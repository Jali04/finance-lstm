{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "362a40d2-85eb-4eb7-9305-75a83695cb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_DIR: ..\\results\\2025-10-19_16-48-09_lstm\n"
     ]
    }
   ],
   "source": [
    "# 03_train_lstm.ipynb  —  Block 3 (mit WFCV-Bestkonfig-Übernahme)\n",
    "\n",
    "import os, sys, json, time, glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = os.path.abspath(\"..\")\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.insert(0, ROOT)\n",
    "\n",
    "# === 0) Grund-Config laden ====================================================\n",
    "with open(os.path.join(ROOT, \"config.json\"), \"r\") as f:\n",
    "    C = json.load(f)\n",
    "\n",
    "TICKER   = C[\"ticker\"]; START = C[\"start\"]; END = C[\"end\"]; INTERVAL = C[\"interval\"]\n",
    "HORIZON  = int(C[\"horizon\"])\n",
    "LOOKBACK = int(C[\"lookback\"])                  # kann von best_config.json überschrieben werden\n",
    "BATCH    = int(C[\"batch\"]); EPOCHS = int(C[\"epochs\"])\n",
    "SEED     = int(C.get(\"seed\", 42))\n",
    "FEATURESET = C.get(\"featureset\", \"v2\")\n",
    "EPS_MODE   = C.get(\"epsilon_mode\", \"abs\")\n",
    "EPSILON    = float(C.get(\"epsilon\", 0.0005))\n",
    "\n",
    "RESULTS_DIR = Path(C.get(\"results_dir\", \"../results\"))\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RUN_DIR   = RESULTS_DIR / time.strftime(\"%Y-%m-%d_%H-%M-%S_lstm\")\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"RUN_DIR:\", RUN_DIR)\n",
    "\n",
    "# Train-CSV wie in Block 2 benannt (inkl. eps_tag)\n",
    "eps_tag   = f\"{EPS_MODE}{str(EPSILON).replace('.','p')}\"\n",
    "TRAIN_CSV = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h{HORIZON}_{eps_tag}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "27a371be-faf9-43d2-8bd0-53399c30ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    balanced_accuracy_score, matthews_corrcoef, average_precision_score,\n",
    "    roc_auc_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2b82cb57-3a32-480c-8945-d35214679cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefunden best_config.json: ..\\results\\2025-10-19_16-45-57_wfcv\\best_config.json\n",
      "[Block3 Setup] cell=GRU width=32/16 dp=0.1 lr=0.0005 lookback=60 | features_used=mom_only\n"
     ]
    }
   ],
   "source": [
    "# === 1) WFCV-best_config.json (neuester Run) laden ============================\n",
    "def _latest_best_config(results_dir=\"../results\"):\n",
    "    \"\"\"Finde den neuesten Ordner, der auf *_wfcv endet, und lese best_config.json.\"\"\"\n",
    "    pattern = os.path.join(results_dir, \"*_wfcv\", \"best_config.json\")\n",
    "    cands = glob.glob(pattern)\n",
    "    if not cands:\n",
    "        return None, None\n",
    "    cands = sorted(cands, key=os.path.getmtime)  # nach Modifikationszeit\n",
    "    best_path = cands[-1]\n",
    "    with open(best_path, \"r\") as f:\n",
    "        best_cfg = json.load(f)\n",
    "    return best_cfg, best_path\n",
    "\n",
    "BEST_CFG, BEST_CFG_PATH = _latest_best_config(RESULTS_DIR)\n",
    "if BEST_CFG is None:\n",
    "    print(\"[INFO] Keine best_config.json gefunden — nutze Fallback (Config.json-Defaults).\")\n",
    "    # Fallback: entspricht deiner bisherigen Default-Architektur\n",
    "    BEST_CFG = {\n",
    "        \"features_used\": \"all\",\n",
    "        \"lookback\": LOOKBACK,\n",
    "        \"cell\": \"GRU\",\n",
    "        \"width1\": 32,\n",
    "        \"width2\": 16,\n",
    "        \"dropout\": 0.10,\n",
    "        \"lr\": 5e-4\n",
    "    }\n",
    "else:\n",
    "    print(\"Gefunden best_config.json:\", BEST_CFG_PATH)\n",
    "\n",
    "# HP aus best_config übernehmen (LOOKBACK ggf. überschreiben)\n",
    "CELL    = str(BEST_CFG.get(\"cell\", \"GRU\")).upper()\n",
    "WIDTH1  = int(BEST_CFG.get(\"width1\", 32))\n",
    "WIDTH2  = int(BEST_CFG.get(\"width2\", 16))\n",
    "DROPOUT = float(BEST_CFG.get(\"dropout\", 0.10))\n",
    "LR      = float(BEST_CFG.get(\"lr\", 5e-4))\n",
    "LB_FROM_BEST = int(BEST_CFG.get(\"lookback\", LOOKBACK))\n",
    "USE_LOOKBACK = LB_FROM_BEST if LB_FROM_BEST > 0 else LOOKBACK\n",
    "\n",
    "FEATURES_USED_TAG = str(BEST_CFG.get(\"features_used\", \"all\"))\n",
    "\n",
    "print(f\"[Block3 Setup] cell={CELL} width={WIDTH1}/{WIDTH2} dp={DROPOUT} lr={LR} lookback={USE_LOOKBACK} \"\n",
    "      f\"| features_used={FEATURES_USED_TAG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "07b91fc5-71ec-46b3-b625-f02038552a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TRAIN_CSV: ../data/AAPL_1d_2012-01-01_2025-09-01_cls_h5_abs0p0005.csv\n",
      "[Label] using horizon=5 | mode=abs | epsilon=0.0005\n",
      "FEATURES (final): ['logret_1d', 'logret_3d', 'logret_5d', 'bb_pos', 'rsi_14', 'macd', 'macd_sig', 'macd_diff', 'sma_diff']\n"
     ]
    }
   ],
   "source": [
    "# === 2) Daten & Features einlesen (robust zur H/ε-Diskrepanz) ===============\n",
    "import yaml, glob, os\n",
    "\n",
    "yaml_path = f\"../data/features_{FEATURESET}.yml\"\n",
    "meta = {}\n",
    "if os.path.exists(yaml_path):\n",
    "    with open(yaml_path, \"r\") as f:\n",
    "        meta = yaml.safe_load(f) or {}\n",
    "\n",
    "def _resolve_train_csv():\n",
    "    # 1) Versuch: exakte Datei gemäß aktueller config.json\n",
    "    eps_tag_cfg = f\"{EPS_MODE}{str(EPSILON).replace('.','p')}\"\n",
    "    exact = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h{HORIZON}_{eps_tag_cfg}.csv\"\n",
    "    if os.path.exists(exact):\n",
    "        return exact\n",
    "\n",
    "    # 2) Versuch: nimm Label-Definition aus YAML (Block 2 ist Quelle der Wahrheit)\n",
    "    lab = (meta or {}).get(\"label\", {})\n",
    "    h_yaml   = int(lab.get(\"horizon\", HORIZON))\n",
    "    mode_yaml= str(lab.get(\"mode\", EPS_MODE))\n",
    "    eps_yaml = float(lab.get(\"epsilon\", EPSILON))\n",
    "    eps_tag_yaml = f\"{mode_yaml}{str(eps_yaml).replace('.','p')}\"\n",
    "    by_yaml = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h{h_yaml}_{eps_tag_yaml}.csv\"\n",
    "    if os.path.exists(by_yaml):\n",
    "        return by_yaml\n",
    "\n",
    "    # 3) Versuch: gleiche H/ε (aus YAML), aber beliebiges h via Glob\n",
    "    pat_same_eps = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h*_{eps_tag_yaml}.csv\"\n",
    "    cands = sorted(glob.glob(pat_same_eps), key=os.path.getmtime)\n",
    "    if cands:\n",
    "        return cands[-1]\n",
    "\n",
    "    # 4) Letzte Instanz: irgendein *_cls_h*.csv für diesen Ticker/Zeitraum\n",
    "    pat_any = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h*.csv\"\n",
    "    cands = sorted(glob.glob(pat_any), key=os.path.getmtime)\n",
    "    if cands:\n",
    "        return cands[-1]\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Kein TRAIN_CSV gefunden.\\n\"\n",
    "        f\"Versucht:\\n- {exact}\\n- {by_yaml}\\n- {pat_same_eps}\\n- {pat_any}\\n\"\n",
    "        \"Hinweis: Block 2 mit identischen HORIZON/epsilon erneut laufen lassen.\"\n",
    "    )\n",
    "\n",
    "TRAIN_CSV = _resolve_train_csv()\n",
    "print(\"Loaded TRAIN_CSV:\", TRAIN_CSV)\n",
    "\n",
    "# --- Label-Parameter (H, mode, epsilon) aus Daten erzwingen -------------------\n",
    "import re\n",
    "\n",
    "def _infer_label_from(meta_dict, train_csv_path, fallback_h_from_root):\n",
    "    h, mode, eps = None, None, None\n",
    "\n",
    "    # 1) YAML als Quelle der Wahrheit (Block 2 schreibt das rein)\n",
    "    if meta_dict and isinstance(meta_dict, dict):\n",
    "        L = (meta_dict.get(\"label\") or {})\n",
    "        if \"horizon\" in L: h = int(L[\"horizon\"])\n",
    "        if \"mode\"    in L: mode = str(L[\"mode\"])\n",
    "        if \"epsilon\" in L:\n",
    "            try:\n",
    "                eps = float(L[\"epsilon\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # 2) Robust: aus Dateinamen …_cls_h{H}_{mode}{eps_tag}.csv parsen\n",
    "    m = re.search(r\"_cls_h(\\d+)_([a-z]+)([0-9p]+)\\.csv$\", str(train_csv_path))\n",
    "    if m:\n",
    "        if h    is None: h    = int(m.group(1))\n",
    "        if mode is None: mode = m.group(2)\n",
    "        if eps  is None:\n",
    "            eps_tag = m.group(3)          # z.B. 0p0005\n",
    "            eps = float(eps_tag.replace(\"p\", \".\"))\n",
    "\n",
    "    # 3) Fallback: notfalls Root-H (sollte eigentlich nie passieren)\n",
    "    if h is None:\n",
    "        h = int(fallback_h_from_root)\n",
    "\n",
    "    return h, mode, eps\n",
    "\n",
    "# erzwingen, dass HORIZON/EPS aus Daten kommen\n",
    "H_DATA, MODE_DATA, EPS_DATA = _infer_label_from(meta, TRAIN_CSV, HORIZON)\n",
    "\n",
    "# Diese Werte ab hier im Run *verbindlich* nutzen\n",
    "HORIZON  = int(H_DATA)\n",
    "if MODE_DATA is not None:   EPS_MODE = str(MODE_DATA)\n",
    "if EPS_DATA  is not None:   EPSILON  = float(EPS_DATA)\n",
    "\n",
    "# für Transparenz\n",
    "print(f\"[Label] using horizon={HORIZON} | mode={EPS_MODE} | epsilon={EPSILON}\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(TRAIN_CSV, index_col=0, parse_dates=True).sort_index()\n",
    "\n",
    "# Features aus YAML ziehen und mit df abgleichen\n",
    "ALL_FEATURES = [c for c in (meta.get(\"features\", []) if meta else []) if c in df.columns]\n",
    "if not ALL_FEATURES:\n",
    "    # Fallback: alles außer OHLCV/target\n",
    "    OHLCV = {\"open\",\"high\",\"low\",\"close\",\"volume\"}\n",
    "    ALL_FEATURES = [c for c in df.columns if c not in (OHLCV | {\"target\"})]\n",
    "assert len(ALL_FEATURES) > 0, f\"Keine nutzbaren Features in {yaml_path} oder df gefunden.\"\n",
    "\n",
    "# Feature-Subset konsistent zu best_config.json\n",
    "if FEATURES_USED_TAG == \"mom_only\":\n",
    "    FEATURES = [c for c in ALL_FEATURES\n",
    "                if (\"logret\" in c) or (\"macd\" in c) or (c in {\"sma_diff\",\"rsi_14\",\"bb_pos\"})]\n",
    "    assert len(FEATURES) > 0, \"Feature-Subset 'mom_only' ist leer — bitte prüfen.\"\n",
    "else:\n",
    "    FEATURES = ALL_FEATURES\n",
    "\n",
    "TARGET = \"target\"\n",
    "X = df[FEATURES].copy()\n",
    "y = df[TARGET].astype(int).copy()\n",
    "print(\"FEATURES (final):\", FEATURES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "af24474a-6165-44f4-9243-a688d6372c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes → train 2381, val 510, test 511\n"
     ]
    }
   ],
   "source": [
    "# === 3) Chronologische Splits (70/15/15) =====================================\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n = len(df)\n",
    "n_train = int(n * 0.70)\n",
    "n_val   = int(n * 0.15)\n",
    "train_idx = slice(0, n_train)\n",
    "val_idx   = slice(n_train, n_train + n_val)\n",
    "test_idx  = slice(n_train + n_val, n)\n",
    "\n",
    "X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "X_val,   y_val   = X.iloc[val_idx],   y.iloc[val_idx]\n",
    "X_test,  y_test  = X.iloc[test_idx],  y.iloc[test_idx]\n",
    "print(f\"Split sizes → train {len(X_train)}, val {len(X_val)}, test {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5c6197b0-c082-48ba-9dd5-6de13a80ed3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape/check: (2381, 9)  | cols: ['logret_1d', 'logret_3d', 'logret_5d', 'bb_pos', 'rsi_14', 'macd', 'macd_sig', 'macd_diff', 'sma_diff']\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape/check:\", X_train.shape, \" | cols:\", list(X_train.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3e6af278-7df7-4bd2-beb7-ad8eea7e7bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\results\\\\2025-10-19_16-48-09_lstm\\\\scaler.joblib']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 4) Scaler nur auf TRAIN fitten ==========================================\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_train_s = pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index, columns=FEATURES)\n",
    "X_val_s   = pd.DataFrame(scaler.transform(X_val),       index=X_val.index,   columns=FEATURES)\n",
    "X_test_s  = pd.DataFrame(scaler.transform(X_test),      index=X_test.index,  columns=FEATURES)\n",
    "\n",
    "import joblib\n",
    "joblib.dump(scaler, RUN_DIR / \"scaler.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2b8a3e5d-4afb-4c23-b4a0-af4234725c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drift-Report (lassen)\n",
    "def drift_summary(Xa: pd.DataFrame, Xb: pd.DataFrame):\n",
    "    out = []\n",
    "    for c in Xa.columns:\n",
    "        m1, s1 = Xa[c].mean(), Xa[c].std(ddof=1)\n",
    "        m2, s2 = Xb[c].mean(), Xb[c].std(ddof=1)\n",
    "        ratio_std = float((s2 + 1e-9) / (s1 + 1e-9))\n",
    "        diff_mean = float(m2 - m1)\n",
    "        out.append({\"feature\": c, \"mean_diff\": diff_mean, \"std_ratio\": ratio_std})\n",
    "    return pd.DataFrame(out).sort_values(\"std_ratio\", ascending=False)\n",
    "\n",
    "drift_df = drift_summary(X_train_s, X_test_s)\n",
    "drift_df.to_csv(RUN_DIR / \"drift_train_vs_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "de930d62-f80e-408a-9471-fcdbb3c2b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5) Windowing & tf.data ===================================================\n",
    "def make_windows(X_df: pd.DataFrame, y_ser: pd.Series, lookback: int):\n",
    "    X_values = X_df.values.astype(np.float32)\n",
    "    y_values = y_ser.values.astype(np.int32)\n",
    "    n = len(X_df)\n",
    "    xs, ys = [], []\n",
    "    for i in range(lookback-1, n):\n",
    "        xs.append(X_values[i - lookback + 1 : i + 1])\n",
    "        ys.append(y_values[i])\n",
    "    return np.stack(xs, axis=0), np.array(ys)\n",
    "\n",
    "Xtr_win, ytr = make_windows(X_train_s, y_train, USE_LOOKBACK)\n",
    "Xva_win, yva = make_windows(X_val_s,   y_val,   USE_LOOKBACK)\n",
    "Xte_win, yte = make_windows(X_test_s,  y_test,  USE_LOOKBACK)\n",
    "\n",
    "import tensorflow as tf\n",
    "np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "def to_ds(X, y, batch, shuffle):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(X), seed=SEED, reshuffle_each_iteration=True)\n",
    "    return ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "ds_train = to_ds(Xtr_win, ytr, BATCH, shuffle=True)\n",
    "ds_val   = to_ds(Xva_win, yva, BATCH, shuffle=False)\n",
    "ds_test  = to_ds(Xte_win, yte, BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "63ab8c75-36ad-464b-b8a6-7d87c4b519d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diag] LogReg AUROC val/test = 0.585/0.446\n"
     ]
    }
   ],
   "source": [
    "# === 6) Diagnose: LogReg (Ranking-Baseline) ==================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef\n",
    "\n",
    "logit = LogisticRegression(max_iter=200)\n",
    "logit.fit(X_train_s.iloc[USE_LOOKBACK-1:], y_train.iloc[USE_LOOKBACK-1:])\n",
    "y_proba_lr = logit.predict_proba(X_test_s.iloc[USE_LOOKBACK-1:])[:,1]\n",
    "print(f\"[Diag] LogReg AUROC val/test = \"\n",
    "      f\"{roc_auc_score(y_val.iloc[USE_LOOKBACK-1:], logit.predict_proba(X_val_s.iloc[USE_LOOKBACK-1:])[:,1]):.3f}/\"\n",
    "      f\"{roc_auc_score(y_test.iloc[USE_LOOKBACK-1:], y_proba_lr):.3f}\")\n",
    "\n",
    "# === 7) Modell nach best_config.json =========================================\n",
    "from tensorflow.keras import layers, regularizers, optimizers, callbacks, models\n",
    "\n",
    "rnn_cell = layers.GRU if CELL == \"GRU\" else layers.LSTM\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(USE_LOOKBACK, len(FEATURES))),\n",
    "    rnn_cell(WIDTH1, return_sequences=True, recurrent_dropout=DROPOUT),\n",
    "    layers.LayerNormalization(),\n",
    "    rnn_cell(WIDTH2, recurrent_dropout=DROPOUT),\n",
    "    layers.LayerNormalization(),\n",
    "    layers.Dense(16, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-5)),\n",
    "    layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=LR),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        tf.keras.metrics.AUC(name=\"auc\"),\n",
    "        tf.keras.metrics.AUC(name=\"auprc\", curve=\"PR\"),\n",
    "        tf.keras.metrics.BinaryAccuracy(name=\"acc\"),\n",
    "        tf.keras.metrics.Precision(name=\"prec\"),\n",
    "        tf.keras.metrics.Recall(name=\"rec\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "ckpt_path = RUN_DIR / \"best.keras\"\n",
    "cbs = [\n",
    "    callbacks.ModelCheckpoint(filepath=str(ckpt_path),\n",
    "                              monitor=\"val_auprc\", mode=\"max\",\n",
    "                              save_best_only=True, verbose=1),\n",
    "    callbacks.EarlyStopping(monitor=\"val_auprc\", mode=\"max\",\n",
    "                            patience=12, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_auprc\", mode=\"max\",\n",
    "                                factor=0.5, patience=6, min_lr=1e-5, verbose=1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "76d25d1b-8215-40c9-9b57-e469739b9fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.5354 - auc: 0.4977 - auprc: 0.5914 - loss: 0.7758 - prec: 0.5888 - rec: 0.7098\n",
      "Epoch 1: val_auprc improved from None to 0.61817, saving model to ..\\results\\2025-10-19_16-48-09_lstm\\best.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - acc: 0.5323 - auc: 0.4977 - auprc: 0.5811 - loss: 0.7447 - prec: 0.5709 - rec: 0.7634 - val_acc: 0.4989 - val_auc: 0.5516 - val_auprc: 0.6182 - val_loss: 0.7149 - val_prec: 0.5171 - val_rec: 0.7605 - learning_rate: 5.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.5330 - auc: 0.5011 - auprc: 0.5790 - loss: 0.6992 - prec: 0.5610 - rec: 0.8028\n",
      "Epoch 2: val_auprc improved from 0.61817 to 0.62571, saving model to ..\\results\\2025-10-19_16-48-09_lstm\\best.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.5543 - auc: 0.5204 - auprc: 0.6033 - loss: 0.6881 - prec: 0.5817 - rec: 0.8104 - val_acc: 0.5410 - val_auc: 0.6087 - val_auprc: 0.6257 - val_loss: 0.6815 - val_prec: 0.5447 - val_rec: 0.7941 - learning_rate: 5.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.5414 - auc: 0.5557 - auprc: 0.6187 - loss: 0.6858 - prec: 0.5569 - rec: 0.8675\n",
      "Epoch 3: val_auprc did not improve from 0.62571\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.5685 - auc: 0.5574 - auprc: 0.6320 - loss: 0.6778 - prec: 0.5851 - rec: 0.8672 - val_acc: 0.5831 - val_auc: 0.6239 - val_auprc: 0.6187 - val_loss: 0.6759 - val_prec: 0.5731 - val_rec: 0.8235 - learning_rate: 5.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.5736 - auc: 0.5802 - auprc: 0.6208 - loss: 0.6791 - prec: 0.5750 - rec: 0.8770\n",
      "Epoch 4: val_auprc did not improve from 0.62571\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.5900 - auc: 0.5815 - auprc: 0.6485 - loss: 0.6703 - prec: 0.5986 - rec: 0.8791 - val_acc: 0.6386 - val_auc: 0.6292 - val_auprc: 0.6158 - val_loss: 0.6716 - val_prec: 0.6133 - val_rec: 0.8529 - learning_rate: 5.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.6007 - auc: 0.6331 - auprc: 0.6707 - loss: 0.6633 - prec: 0.5897 - rec: 0.9262\n",
      "Epoch 5: val_auprc did not improve from 0.62571\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.5965 - auc: 0.6047 - auprc: 0.6666 - loss: 0.6641 - prec: 0.6020 - rec: 0.8873 - val_acc: 0.6164 - val_auc: 0.6185 - val_auprc: 0.6105 - val_loss: 0.6728 - val_prec: 0.5964 - val_rec: 0.8445 - learning_rate: 5.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.6020 - auc: 0.6183 - auprc: 0.6769 - loss: 0.6613 - prec: 0.6026 - rec: 0.9106\n",
      "Epoch 6: val_auprc did not improve from 0.62571\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.5986 - auc: 0.6145 - auprc: 0.6751 - loss: 0.6611 - prec: 0.6026 - rec: 0.8940 - val_acc: 0.6231 - val_auc: 0.6301 - val_auprc: 0.6143 - val_loss: 0.6711 - val_prec: 0.6024 - val_rec: 0.8403 - learning_rate: 5.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.6155 - auc: 0.6388 - auprc: 0.7023 - loss: 0.6538 - prec: 0.6210 - rec: 0.8804\n",
      "Epoch 7: val_auprc did not improve from 0.62571\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6102 - auc: 0.6273 - auprc: 0.6851 - loss: 0.6570 - prec: 0.6160 - rec: 0.8619 - val_acc: 0.6297 - val_auc: 0.6248 - val_auprc: 0.6045 - val_loss: 0.6719 - val_prec: 0.6047 - val_rec: 0.8613 - learning_rate: 5.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.6243 - auc: 0.6789 - auprc: 0.7201 - loss: 0.6464 - prec: 0.6172 - rec: 0.8929\n",
      "Epoch 8: val_auprc did not improve from 0.62571\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.6163 - auc: 0.6426 - auprc: 0.6953 - loss: 0.6526 - prec: 0.6187 - rec: 0.8731 - val_acc: 0.6231 - val_auc: 0.6247 - val_auprc: 0.6114 - val_loss: 0.6703 - val_prec: 0.6024 - val_rec: 0.8403 - learning_rate: 5.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.6231 - auc: 0.6434 - auprc: 0.6892 - loss: 0.6545 - prec: 0.6132 - rec: 0.9099\n",
      "Epoch 9: val_auprc did not improve from 0.62571\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6202 - auc: 0.6526 - auprc: 0.7046 - loss: 0.6494 - prec: 0.6184 - rec: 0.8925 - val_acc: 0.6253 - val_auc: 0.6278 - val_auprc: 0.6092 - val_loss: 0.6691 - val_prec: 0.6062 - val_rec: 0.8277 - learning_rate: 2.5000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.6318 - auc: 0.6748 - auprc: 0.7202 - loss: 0.6446 - prec: 0.6257 - rec: 0.8896\n",
      "Epoch 10: val_auprc did not improve from 0.62571\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6202 - auc: 0.6589 - auprc: 0.7096 - loss: 0.6472 - prec: 0.6191 - rec: 0.8881 - val_acc: 0.6231 - val_auc: 0.6264 - val_auprc: 0.6156 - val_loss: 0.6698 - val_prec: 0.6037 - val_rec: 0.8319 - learning_rate: 2.5000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.6356 - auc: 0.6661 - auprc: 0.7204 - loss: 0.6426 - prec: 0.6326 - rec: 0.8884\n",
      "Epoch 11: val_auprc did not improve from 0.62571\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6365 - auc: 0.6653 - auprc: 0.7151 - loss: 0.6442 - prec: 0.6311 - rec: 0.8910 - val_acc: 0.6275 - val_auc: 0.6262 - val_auprc: 0.6125 - val_loss: 0.6692 - val_prec: 0.6048 - val_rec: 0.8487 - learning_rate: 2.5000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.6421 - auc: 0.6610 - auprc: 0.7205 - loss: 0.6412 - prec: 0.6416 - rec: 0.8877\n",
      "Epoch 12: val_auprc did not improve from 0.62571\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6322 - auc: 0.6671 - auprc: 0.7154 - loss: 0.6425 - prec: 0.6284 - rec: 0.8873 - val_acc: 0.6231 - val_auc: 0.6254 - val_auprc: 0.6142 - val_loss: 0.6716 - val_prec: 0.6012 - val_rec: 0.8487 - learning_rate: 2.5000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - acc: 0.6332 - auc: 0.6625 - auprc: 0.7009 - loss: 0.6471 - prec: 0.6257 - rec: 0.8738\n",
      "Epoch 13: val_auprc did not improve from 0.62571\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.6387 - auc: 0.6642 - auprc: 0.7145 - loss: 0.6423 - prec: 0.6389 - rec: 0.8597 - val_acc: 0.6186 - val_auc: 0.6254 - val_auprc: 0.6155 - val_loss: 0.6702 - val_prec: 0.6006 - val_rec: 0.8277 - learning_rate: 2.5000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.6405 - auc: 0.6800 - auprc: 0.7434 - loss: 0.6347 - prec: 0.6410 - rec: 0.8806\n",
      "Epoch 14: val_auprc did not improve from 0.62571\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6339 - auc: 0.6699 - auprc: 0.7213 - loss: 0.6408 - prec: 0.6295 - rec: 0.8888 - val_acc: 0.6186 - val_auc: 0.6238 - val_auprc: 0.6158 - val_loss: 0.6719 - val_prec: 0.5994 - val_rec: 0.8361 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "# === 8) Training ==============================================================\n",
    "history = model.fit(ds_train, validation_data=ds_val, epochs=EPOCHS,\n",
    "                    callbacks=cbs, verbose=1)\n",
    "pd.DataFrame(history.history).to_csv(RUN_DIR / \"history.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "724d38b3-fbf9-4be4-bb3b-e4fd2f0c6da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 9) Test (roh, ohne Kalibrierung/Final-Threshold) ========================\n",
    "from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bab3b51b-1bef-43b3-a014-87b78d9cc97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test (keras) metrics: {\n",
      "  \"acc\": 0.4690265357494354,\n",
      "  \"auc\": 0.47294431924819946,\n",
      "  \"auprc\": 0.5432189702987671,\n",
      "  \"loss\": 0.7261859178543091,\n",
      "  \"prec\": 0.49666666984558105,\n",
      "  \"rec\": 0.6260504126548767\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Metriken im Keras-Sinne\n",
    "test_metrics = model.evaluate(ds_test, return_dict=True, verbose=0)\n",
    "print(\"Test (keras) metrics:\", json.dumps(test_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3e5f6953-d02c-449f-950b-a667ea307c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diag] thr@val(max MCC, bounds 0.35–0.65) = 0.594 | val_MCC=0.240\n"
     ]
    }
   ],
   "source": [
    "# Diagnose-Threshold: val-MCC innerhalb 0.35–0.65 PosRate (nur Diagnose!)\n",
    "val_proba = model.predict(ds_val, verbose=0).ravel()\n",
    "def choose_threshold(y_true, y_prob, bounds=(0.35, 0.65)):\n",
    "    uniq = np.unique(y_prob); cand = np.r_[0.0, uniq, 1.0]\n",
    "    best_t, best_s = 0.5, -1\n",
    "    for t in cand:\n",
    "        yp = (y_prob >= t).astype(int)\n",
    "        pr = yp.mean()\n",
    "        if not (bounds[0] <= pr <= bounds[1]): \n",
    "            continue\n",
    "        s = matthews_corrcoef(y_true, yp)\n",
    "        if s > best_s: best_s, best_t = s, float(t)\n",
    "    return best_t, best_s\n",
    "\n",
    "thr_diag, mcc_val_diag = choose_threshold(yva, val_proba, bounds=(0.35, 0.65))\n",
    "print(f\"[Diag] thr@val(max MCC, bounds 0.35–0.65) = {thr_diag:.3f} | val_MCC={mcc_val_diag:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8475b950-8eca-4cbc-9fcc-8e4d79c663c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proba stats (test): min= 0.3109811544418335 max= 0.7283353805541992 mean= 0.5499657988548279\n",
      "AUROC val/test: 0.61 / 0.473\n",
      "\n",
      "[Diag] Confusion (test, thr=thr_diag):\n",
      " [[121  93]\n",
      " [139  99]]\n",
      "\n",
      "[Diag] Report (test):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.465     0.565     0.511       214\n",
      "           1      0.516     0.416     0.460       238\n",
      "\n",
      "    accuracy                          0.487       452\n",
      "   macro avg      0.491     0.491     0.486       452\n",
      "weighted avg      0.492     0.487     0.484       452\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test-Probas & Diagnose-Report bei thr_diag (NICHT final reporten; Block 4 entscheidet!)\n",
    "y_proba = model.predict(ds_test, verbose=0).ravel()\n",
    "y_pred_diag = (y_proba >= thr_diag).astype(int)\n",
    "\n",
    "print(\"Proba stats (test): min=\", float(y_proba.min()),\n",
    "      \"max=\", float(y_proba.max()), \"mean=\", float(y_proba.mean()))\n",
    "print(\"AUROC val/test:\",\n",
    "      round(roc_auc_score(yva, val_proba), 3), \"/\",\n",
    "      round(roc_auc_score(yte, y_proba), 3))\n",
    "\n",
    "print(\"\\n[Diag] Confusion (test, thr=thr_diag):\\n\", confusion_matrix(yte, y_pred_diag))\n",
    "print(\"\\n[Diag] Report (test):\\n\", classification_report(yte, y_pred_diag, digits=3))\n",
    "\n",
    "# Extra Test-Metriken (diagnostisch)\n",
    "bal_acc = balanced_accuracy_score(yte, y_pred_diag)\n",
    "mcc     = matthews_corrcoef(yte, y_pred_diag)\n",
    "auprc_t = average_precision_score(yte, y_proba)\n",
    "extra = {\"balanced_accuracy\": float(bal_acc),\n",
    "         \"mcc\": float(mcc),\n",
    "         \"auprc\": float(auprc_t)}\n",
    "with open(RUN_DIR / \"extra_test_metrics_diag.json\", \"w\") as f:\n",
    "    json.dump(extra, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "819bd95d-c17c-4736-9d00-4143c55fbfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 10) Artefakte & Umgebungsinfo (ein konsistenter Dump) ===================\n",
    "env_info = {\n",
    "    \"python\": sys.version,\n",
    "    \"tensorflow\": tf.__version__,\n",
    "    \"seed\": SEED,\n",
    "    \"ticker\": TICKER, \"start\": START, \"end\": END, \"interval\": INTERVAL,\n",
    "    \"horizon\": HORIZON, \"epsilon_mode\": EPS_MODE, \"epsilon\": EPSILON,\n",
    "    \"featureset\": FEATURESET, \"features_used\": FEATURES_USED_TAG,\n",
    "    \"features_final\": FEATURES,\n",
    "    \"lookback\": USE_LOOKBACK, \"batch\": BATCH, \"epochs\": EPOCHS,\n",
    "    \"cell\": CELL, \"width1\": WIDTH1, \"width2\": WIDTH2,\n",
    "    \"dropout\": DROPOUT, \"lr\": LR,\n",
    "    \"loss\": \"BCE\",\n",
    "    \"train_csv\": TRAIN_CSV,\n",
    "    \"features_yaml\": yaml_path,\n",
    "    \"best_config_path\": BEST_CFG_PATH,\n",
    "    \"best_checkpoint_path\": str(ckpt_path)\n",
    "}\n",
    "with open(RUN_DIR / \"env_info.json\", \"w\") as f:\n",
    "    json.dump(env_info, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e4e23272-c1ef-449b-976e-c7b17fd1908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kompakter Config-Dump (einmalig!) — keine Doppel-Schreiberei\n",
    "final_cfg_dump = {\n",
    "    \"ticker\": TICKER, \"start\": START, \"end\": END, \"interval\": INTERVAL,\n",
    "    \"horizon\": HORIZON, \"lookback\": USE_LOOKBACK,\n",
    "    \"featureset\": FEATURESET, \"features\": FEATURES,\n",
    "    \"scaler\": \"StandardScaler\", \"seed\": SEED, \"batch\": BATCH, \"epochs\": EPOCHS,\n",
    "    \"cell\": CELL, \"width1\": WIDTH1, \"width2\": WIDTH2,\n",
    "    \"dropout\": DROPOUT, \"lr\": LR,\n",
    "    \"loss\": \"BCE\",\n",
    "    \"epsilon_mode\": EPS_MODE, \"epsilon\": EPSILON,\n",
    "    \"train_csv\": TRAIN_CSV,                         \n",
    "    \"features_yaml\": yaml_path,\n",
    "    \"wfcv_best_config_source\": BEST_CFG_PATH\n",
    "}\n",
    "with open(RUN_DIR / \"config.json\", \"w\") as f:\n",
    "    json.dump(final_cfg_dump, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "978a8ca1-2094-4e71-bce5-33df526f1e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Artefakte gespeichert in: ..\\results\\2025-10-19_16-48-09_lstm\n"
     ]
    }
   ],
   "source": [
    "# Keras-Export & Rohdaten für Block 4\n",
    "model.save(RUN_DIR / \"model.keras\")\n",
    "np.save(RUN_DIR / \"y_test.npy\", yte)\n",
    "np.save(RUN_DIR / \"y_proba.npy\", y_proba)\n",
    "\n",
    "print(f\"\\nArtefakte gespeichert in: {RUN_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c21f136-1586-402a-b44a-e1275cce874a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (finance-lstm)",
   "language": "python",
   "name": "finance-lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
