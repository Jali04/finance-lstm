{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5df2c55f-8949-47a2-94b9-bb1bc25587a1",
   "metadata": {},
   "source": [
    "# -----------------------------------------\n",
    "# Block 4: Threshold-Tuning, Evaluation, Backtest & Plots\n",
    "# -----------------------------------------\n",
    "# Lädt die Artefakte aus dem letzten RUN_DIR, rekonstruiert\n",
    "# Val/Test-Splits, wählt einen optimalen Schwellenwert auf Val\n",
    "# (max. F1), evaluiert auf Test, erstellt Plots und einfachen Backtest.\n",
    "# -----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4c69ddd0-9c11-473b-a3c9-56496d3485ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, time, glob\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score,\n",
    "    classification_report, confusion_matrix, brier_score_loss,\n",
    "    balanced_accuracy_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c0c90c28-8087-4221-9a3e-466d5312eb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_DIR: ..\\results\\2025-10-03_13-48-57_lstm\n"
     ]
    }
   ],
   "source": [
    "# --- Setup & Config laden ---\n",
    "ROOT = os.path.abspath(\"..\")\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.insert(0, ROOT)\n",
    "\n",
    "with open(os.path.join(ROOT, \"config.json\"), \"r\") as f:\n",
    "    C = json.load(f)\n",
    "\n",
    "TICKER   = C[\"ticker\"]; START = C[\"start\"]; END = C[\"end\"]; INTERVAL = C[\"interval\"]\n",
    "HORIZON  = int(C[\"horizon\"]); LOOKBACK = int(C[\"lookback\"])\n",
    "BATCH    = int(C[\"batch\"]);   EPOCHS   = int(C[\"epochs\"])\n",
    "SEED     = int(C.get(\"seed\", 42))\n",
    "\n",
    "RESULTS_DIR = Path(C.get(\"results_dir\", \"../results\"))\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _latest_run_dir(results_dir: Path) -> Path:\n",
    "    runs = sorted(results_dir.glob(\"*_lstm\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not runs:\n",
    "        raise FileNotFoundError(\"Kein RUN_DIR gefunden. Bitte Block 3 trainieren.\")\n",
    "    return runs[0]\n",
    "\n",
    "RUN_DIR = _latest_run_dir(RESULTS_DIR)\n",
    "print(\"RUN_DIR:\", RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "70085932-0e7c-4267-a84a-55b823b98d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Artefakte laden ---\n",
    "MODEL_PATH  = (RUN_DIR / \"best.keras\") if (RUN_DIR / \"best.keras\").exists() else (RUN_DIR / \"model.keras\")\n",
    "SCALER_PATH = RUN_DIR / \"scaler.joblib\"\n",
    "CFG_PATH    = RUN_DIR / \"config.json\"\n",
    "assert MODEL_PATH.exists() and SCALER_PATH.exists() and CFG_PATH.exists(), \"Fehlende Artefakte.\"\n",
    "\n",
    "with open(CFG_PATH, \"r\") as f:\n",
    "    RCFG = json.load(f)\n",
    "\n",
    "# Wichtig: compile=False\n",
    "model  = keras.models.load_model(MODEL_PATH, compile=False)\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "\n",
    "# --- Daten laden (aus Block 2 erzeugte CSV) ---\n",
    "TRAIN_CSV = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h{HORIZON}.csv\"\n",
    "df = pd.read_csv(TRAIN_CSV, index_col=0, parse_dates=True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0753df2d-817e-405e-ab22-a66be6d3f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-Spalten bestimmen (alle außer OHLCV + target, ggf. durch YAML/RCFG eingeschränkt)\n",
    "OHLCV = {\"open\",\"high\",\"low\",\"close\",\"volume\"}\n",
    "candidates = [c for c in df.columns if c not in (OHLCV | {\"target\"})]\n",
    "if FEATURES:\n",
    "    FEATURES = [c for c in FEATURES if c in candidates]\n",
    "else:\n",
    "    FEATURES = candidates\n",
    "assert len(FEATURES) > 0, f\"Keine Features gefunden. Kandidaten: {candidates}\"\n",
    "\n",
    "X = df[FEATURES].copy()\n",
    "y = df[\"target\"].astype(int).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "24824a33-b6a3-4b61-b363-2551d40fda8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Chronologische Splits (70/15/15) ---\n",
    "n = len(df)\n",
    "n_train = int(n * 0.70)\n",
    "n_val   = int(n * 0.15)\n",
    "n_test  = n - n_train - n_val\n",
    "\n",
    "train_idx = slice(0, n_train)\n",
    "val_idx   = slice(n_train, n_train + n_val)\n",
    "test_idx  = slice(n_train + n_val, n)\n",
    "\n",
    "X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "X_val,   y_val   = X.iloc[val_idx],   y.iloc[val_idx]\n",
    "X_test,  y_test  = X.iloc[test_idx],  y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6584d486-4b95-4f2c-a270-0fd2c5fc839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Skalierung (nur TRAIN fit – wie in Block 3) ---\n",
    "X_train_s = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=FEATURES)\n",
    "X_val_s   = pd.DataFrame(scaler.transform(X_val),   index=X_val.index,   columns=FEATURES)\n",
    "X_test_s  = pd.DataFrame(scaler.transform(X_test),  index=X_test.index,  columns=FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d188f632-449d-4c21-848f-09c43cddd753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURES (eval): ['logret_1d', 'sma_diff']\n"
     ]
    }
   ],
   "source": [
    "# --- Konsistenz-Checks: Featureliste vs. Modell ---\n",
    "# 1) Reihenfolge festnageln (damit Scaler/Modell exakt dieselbe Spaltenreihenfolge sehen)\n",
    "FEATURES = list(FEATURES)\n",
    "print(\"FEATURES (eval):\", FEATURES)\n",
    "\n",
    "# 2) Model-Input-Dimension prüfen\n",
    "model_in = getattr(model.input_shape, \"__iter__\", None)\n",
    "in_dim = model.input_shape[-1] if model_in else model.layers[0].input_shape[-1]\n",
    "assert in_dim == len(FEATURES), (\n",
    "    f\"Feature-Mismatch: Model expects {in_dim} features, but FEATURES has {len(FEATURES)}.\\n\"\n",
    "    \"→ Dies passiert z.B., wenn du Block 2/3 mit einem anderen Feature-Set neu trainiert hast, \"\n",
    "    \"aber hier noch ein älteres RUN_DIR verwendest.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9aebd466-487b-4b3f-b3c5-f3dad3a88c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Windowing (wie Block 3) ---\n",
    "def make_windows(X_df: pd.DataFrame, y_ser: pd.Series, lookback: int):\n",
    "    X_values = X_df.values.astype(np.float32)\n",
    "    y_values = y_ser.values.astype(np.int32)\n",
    "    n = len(X_df)\n",
    "    xs, ys, idx_end = [], [], []\n",
    "    for i in range(lookback-1, n):\n",
    "        xs.append(X_values[i - lookback + 1 : i + 1])\n",
    "        ys.append(y_values[i])\n",
    "        idx_end.append(X_df.index[i])  # Endzeitpunkt des Fensters\n",
    "    return np.stack(xs, axis=0), np.array(ys), pd.DatetimeIndex(idx_end)\n",
    "\n",
    "Xtr_win, ytr, idx_tr = make_windows(X_train_s, y_train, LOOKBACK)\n",
    "Xva_win, yva, idx_va = make_windows(X_val_s,   y_val,   LOOKBACK)\n",
    "Xte_win, yte, idx_te = make_windows(X_test_s,  y_test,  LOOKBACK)\n",
    "\n",
    "def to_ds(X, y, batch, shuffle):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(X), seed=SEED, reshuffle_each_iteration=False)\n",
    "    return ds.batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "ds_val  = to_ds(Xva_win, yva, BATCH, shuffle=False)\n",
    "ds_test = to_ds(Xte_win, yte, BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c71bb7ef-7955-4b2f-b901-68f04327ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Roh-Wahrscheinlichkeiten ---\n",
    "y_val_proba  = model.predict(ds_val,  verbose=0).ravel()\n",
    "y_test_proba = model.predict(ds_test, verbose=0).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa83ef-1c86-4f18-aea8-587cfbdd1721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8f1ac0c3-a5b9-4799-bff4-7eff4450611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalibrator gewählt: isotonic | Brier(val) iso=0.2440, platt=0.2498\n",
      "Threshold@ER(val) = 0.500 | E[ret]_val=0.000813\n"
     ]
    }
   ],
   "source": [
    "# === Kalibrierung: Isotonic vs. Platt (Temperature-like) ======================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "# 1) Isotonic (wie gehabt)\n",
    "iso = IsotonicRegression(out_of_bounds=\"clip\").fit(y_val_proba, yva)\n",
    "val_iso  = iso.transform(y_val_proba)\n",
    "test_iso = iso.transform(y_test_proba)\n",
    "\n",
    "# 2) Platt/Temperature-ähnlich: Logistic Regr. auf Logits\n",
    "#    (entspricht sigmoid(a*logit(p)+b)); stabiler als isotonic bei knapper Datenlage.\n",
    "logit_val = logit(np.clip(y_val_proba, 1e-6, 1-1e-6))\n",
    "lr_platt = LogisticRegression(solver=\"lbfgs\")\n",
    "lr_platt.fit(logit_val.reshape(-1,1), yva)\n",
    "def platt_transform(p):\n",
    "    z = logit(np.clip(p, 1e-6, 1-1e-6)).reshape(-1,1)\n",
    "    return lr_platt.predict_proba(z)[:,1]\n",
    "\n",
    "val_platt  = platt_transform(y_val_proba)\n",
    "test_platt = platt_transform(y_test_proba)\n",
    "\n",
    "# Wähle Kalibrator per Val-Brier\n",
    "brier_iso   = brier_score_loss(yva, val_iso)\n",
    "brier_platt = brier_score_loss(yva, val_platt)\n",
    "use_platt = brier_platt <= brier_iso\n",
    "y_val_cal  = val_platt  if use_platt else val_iso\n",
    "y_test_cal = test_platt if use_platt else test_iso\n",
    "calib_name = \"platt\" if use_platt else \"isotonic\"\n",
    "\n",
    "joblib.dump({\"type\": calib_name, \"iso\": iso if not use_platt else None,\n",
    "             \"platt_coef\": (float(lr_platt.coef_[0,0]), float(lr_platt.intercept_[0]))},\n",
    "            RUN_DIR / \"calibrator.pkl\")\n",
    "\n",
    "print(f\"Kalibrator gewählt: {calib_name} | Brier(val) iso={brier_iso:.4f}, platt={brier_platt:.4f}\")\n",
    "\n",
    "# === Schwelle via Expected Return auf VAL (mit Pos-Rate-Bounds) ===============\n",
    "def choose_thr_expected_return(y_true, y_prob, fwd_ret, bounds=(0.2,0.8)):\n",
    "    uniq = np.unique(y_prob); cand = np.r_[0.0, uniq, 1.0]\n",
    "    best_t, best_er = 0.5, -1e9\n",
    "    for t in cand:\n",
    "        sig = (y_prob >= t).astype(int)\n",
    "        pr = sig.mean()\n",
    "        if not (bounds[0] <= pr <= bounds[1]): continue\n",
    "        er = float((sig * fwd_ret).mean())      # erwartete Rendite\n",
    "        if er > best_er: best_er, best_t = er, float(t)\n",
    "    return best_t, best_er\n",
    "\n",
    "# fwd-returns an die Fenster-Endindizes der Val/Tests anlegen\n",
    "close = df[\"close\"].copy()\n",
    "fwd_val = (np.log(close.shift(-HORIZON)) - np.log(close)).reindex(idx_va)\n",
    "fwd_tst = (np.log(close.shift(-HORIZON)) - np.log(close)).reindex(idx_te)\n",
    "\n",
    "thr, er_val = choose_thr_expected_return(yva, y_val_cal, fwd_val.values, bounds=(0.2,0.8))\n",
    "print(f\"Threshold@ER(val) = {thr:.3f} | E[ret]_val={er_val:.6f}\")\n",
    "\n",
    "# (verhindert jegliche Off-by-…-Fehler)\n",
    "y_test_from_ds = np.concatenate([y.numpy() for _, y in ds_test], axis=0).astype(int)\n",
    "\n",
    "y_test_proba  = model.predict(ds_test, verbose=0).ravel()\n",
    "# Falls du schon kalibrierst:\n",
    "y_test_proba_cal = calibrator.transform(y_test_proba)\n",
    "\n",
    "# Sicherstellen, dass alles passt\n",
    "assert len(y_test_from_ds) == len(y_test_proba_cal), (\n",
    "    f\"len(y_true)={len(y_test_from_ds)} vs len(proba)={len(y_test_proba_cal)}\"\n",
    ")\n",
    "\n",
    "# Threshold anwenden\n",
    "y_test_pred = (y_test_proba_cal >= thr).astype(int)\n",
    "\n",
    "# Jetzt Confusion Matrix etc.\n",
    "cm  = confusion_matrix(y_test_from_ds, y_test_pred)\n",
    "rep = classification_report(y_test_from_ds, y_test_pred, digits=3, output_dict=True)\n",
    "fpr, tpr, _ = roc_curve(yte, y_test_cal); roc_auc = auc(fpr, tpr)\n",
    "prec, rec, _ = precision_recall_curve(yte, y_test_cal); ap = average_precision_score(yte, y_test_cal)\n",
    "brier = brier_score_loss(yte, y_test_cal)\n",
    "bal_acc = balanced_accuracy_score(yte, y_test_pred)\n",
    "mcc = matthews_corrcoef(yte, y_test_pred)\n",
    "\n",
    "# === Decile-Lift & Precision@k ===============================================\n",
    "def decile_lift(y_true, y_prob, k=10):\n",
    "    dfp = pd.DataFrame({\"y\":y_true, \"p\":y_prob}).sort_values(\"p\", ascending=False)\n",
    "    dfp[\"decile\"] = np.ceil((np.arange(len(dfp))+1) / (len(dfp)/k)).astype(int).clip(1,k)\n",
    "    grp = dfp.groupby(\"decile\")[\"y\"].agg([\"mean\",\"count\"]).rename(columns={\"mean\":\"pos_rate\"})\n",
    "    lift = grp[\"pos_rate\"] / dfp[\"y\"].mean()\n",
    "    grp[\"lift\"] = lift\n",
    "    return grp\n",
    "\n",
    "def precision_at_k(y_true, y_prob, ks=(25,50,100)):\n",
    "    order = np.argsort(-y_prob)\n",
    "    out = {}\n",
    "    for k in ks:\n",
    "        sel = order[:min(k, len(order))]\n",
    "        out[f\"P@{k}\"] = float(y_true[sel].mean())\n",
    "    return out\n",
    "\n",
    "deciles = decile_lift(yte, y_test_cal, 10)\n",
    "p_at = precision_at_k(yte, y_test_cal, (25,50,100))\n",
    "\n",
    "deciles.to_csv(RUN_DIR / \"decile_lift_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "df77d65d-26fe-4609-898c-800dee5c857d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (test):\n",
      " [[ 61 147]\n",
      " [ 63 181]]\n",
      "MCC: 0.039 BalancedAcc: 0.518 AUROC: 0.519 AUPRC: 0.55\n"
     ]
    }
   ],
   "source": [
    "# --- Test-Evaluation (kalibriert @ thr) ---\n",
    "y_test_pred = (y_test_proba_cal >= thr).astype(int)\n",
    "\n",
    "cm   = confusion_matrix(yte, y_test_pred)\n",
    "rep  = classification_report(yte, y_test_pred, digits=3, output_dict=True)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(yte, y_test_proba_cal)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(yte, y_test_proba_cal)\n",
    "ap = average_precision_score(yte, y_test_proba_cal)\n",
    "\n",
    "brier   = brier_score_loss(yte, y_test_proba_cal)\n",
    "bal_acc = balanced_accuracy_score(yte, y_test_pred)\n",
    "mcc     = matthews_corrcoef(yte, y_test_pred)\n",
    "\n",
    "print(\"Confusion matrix (test):\\n\", cm)\n",
    "print(\"MCC:\", round(mcc,3), \"BalancedAcc:\", round(bal_acc,3), \"AUROC:\", round(roc_auc,3), \"AUPRC:\", round(ap,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e1404cda-f83e-43b6-8e8b-e31d868a1ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC Bootstrap CI [2.5,50,97.5]: [np.float64(-0.046), np.float64(0.029), np.float64(0.107)]\n"
     ]
    }
   ],
   "source": [
    "# --- (Optional) Block-Bootstrap-CI für MCC (zeitsensitiv) ---\n",
    "rng = np.random.default_rng(SEED)\n",
    "def block_bootstrap_mcc(y_true, y_prob, threshold, n=300, block=LOOKBACK):\n",
    "    idx = np.arange(len(y_true))\n",
    "    scores = []\n",
    "    for _ in range(n):\n",
    "        starts = rng.integers(0, max(1, len(idx)-block+1), size=max(1, len(idx)//block))\n",
    "        bs = np.concatenate([np.arange(s, min(s+block, len(idx))) for s in starts])\n",
    "        yp = (y_prob[bs] >= threshold).astype(int)\n",
    "        scores.append(matthews_corrcoef(y_true[bs], yp))\n",
    "    return np.percentile(scores, [2.5, 50, 97.5]).astype(float)\n",
    "\n",
    "mcc_ci = block_bootstrap_mcc(yte, y_test_proba_cal, thr, n=300, block=LOOKBACK)\n",
    "print(\"MCC Bootstrap CI [2.5,50,97.5]:\", [round(x,3) for x in mcc_ci])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "262a19eb-6fcf-415c-80ae-3eb318ce44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plots ---\n",
    "FIG_DIR = RUN_DIR / \"figures\"\n",
    "FIG_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC={roc_auc:.3f}\")\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC (Test)\"); plt.legend()\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR / \"roc_test.png\", dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rec, prec, label=f\"AP={ap:.3f}\")\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Precision-Recall (Test)\"); plt.legend()\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR / \"pr_test.png\", dpi=160); plt.close()\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(yte, y_test_proba_cal, n_bins=10, strategy=\"quantile\")\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\")\n",
    "plt.xlabel(\"Vorhergesagt\"); plt.ylabel(\"Tatsächlich\"); plt.title(\"Kalibrierung (Test)\")\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR / \"calibration_test.png\", dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(4.5,4))\n",
    "plt.imshow(cm, interpolation=\"nearest\"); plt.title(\"Confusion Matrix (Test)\"); plt.colorbar()\n",
    "ticks = np.arange(2); plt.xticks(ticks, [\"0\",\"1\"]); plt.yticks(ticks, [\"0\",\"1\"])\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR / \"cm_test.png\", dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(y_test_proba_cal, bins=30)\n",
    "plt.axvline(thr, linestyle=\"--\")\n",
    "plt.title(\"P(y=1) Verteilung (Test, kalibriert)\")\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR / \"proba_hist_test.png\", dpi=160); plt.close()\n",
    "\n",
    "pos_rate_test = float(yte.mean())\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rec, prec, label=f\"AP={ap:.3f}\")\n",
    "plt.hlines(pos_rate_test, xmin=0, xmax=1, linestyles=\"--\", label=f\"Baseline={pos_rate_test:.3f}\")\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Precision-Recall (Test)\")\n",
    "plt.legend()\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR / \"pr_test.png\", dpi=160); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "08cfd2af-70a6-4187-ab6f-c0ffb4d3c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Predictions-CSV (Test, kalibriert) ---\n",
    "preds_test = pd.DataFrame({\n",
    "    \"timestamp\": idx_te,\n",
    "    \"y_true\": yte,\n",
    "    \"y_proba_cal\": y_test_proba_cal,\n",
    "    \"y_pred\": y_test_pred,\n",
    "}).set_index(\"timestamp\")\n",
    "preds_test.to_csv(RUN_DIR / \"preds_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ac31e9e3-bb48-4f08-b07b-01adcb99f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Einfache Handels-Strategie & Backtest (kalibriert @ thr) ---\n",
    "close = df[\"close\"].copy()\n",
    "fwd_logret = (np.log(close.shift(-HORIZON)) - np.log(close)).reindex(idx_te)\n",
    "\n",
    "# --- Konservative Variante: Entry am Folgetag (T+1) ---\n",
    "signals_t = (preds_test[\"y_proba_cal\"] >= thr).astype(int).reindex(idx_te)\n",
    "signals_t1 = signals_t.shift(1).fillna(0)  # erster Tag kein Trade\n",
    "fwd_logret_t1 = (np.log(close.shift(-HORIZON)) - np.log(close)).reindex(idx_te)\n",
    "\n",
    "equity_t1 = (signals_t1 * fwd_logret_t1).fillna(0).cumsum().apply(np.exp)\n",
    "\n",
    "def _sharpe(logrets, periods_per_year=252):\n",
    "    if len(logrets) < 2: return float(\"nan\")\n",
    "    mu = logrets.mean() * periods_per_year\n",
    "    sigma = logrets.std(ddof=1) * np.sqrt(periods_per_year)\n",
    "    return float(mu / (sigma + 1e-12))\n",
    "\n",
    "def _cagr(equity_series, periods_per_year=252):\n",
    "    if len(equity_series) < 2: return float(\"nan\")\n",
    "    T = len(equity_series) / periods_per_year\n",
    "    return float((equity_series.iloc[-1] / equity_series.iloc[0])**(1.0/T) - 1.0)\n",
    "\n",
    "backtest = {\n",
    "    \"n_trades\": int(signals.sum()),\n",
    "    \"avg_holding_h\": HORIZON,\n",
    "    \"strategy\": {\n",
    "        \"CAGR\": _cagr(equity),\n",
    "        \"Sharpe\": _sharpe(strategy_logret.dropna()),\n",
    "        \"final_equity\": float(equity.iloc[-1]),\n",
    "    },\n",
    "    \"buy_hold\": {\n",
    "        \"CAGR\": _cagr(bh_equity),\n",
    "        \"final_equity\": float(bh_equity.iloc[-1]),\n",
    "    },\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(equity.index, equity.values, label=\"Entry@t (optimistisch)\")\n",
    "plt.plot(equity_t1.index, equity_t1.values, label=\"Entry@t+1 (konservativ)\")\n",
    "plt.plot(bh_equity.index, bh_equity.values, label=\"Buy & Hold\", linestyle=\"--\")\n",
    "plt.title(f\"Equity Curves (H={HORIZON})\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"equity_curves_t_vs_t1.png\", dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.boxplot([fwd_logret[signals==0].dropna(), fwd_logret[signals==1].dropna()], tick_labels=[\"Signal=0\",\"Signal=1\"])\n",
    "plt.title(\"Forward Log-Return nach Signal\")\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR / \"forward_returns_by_signal.png\", dpi=160); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e0a1c3dd-3650-4c1e-845e-63c98a6421e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brier raw=0.2504 → calibrated=0.2484\n",
      "Precision@k: {'P@25': 0.6, 'P@50': 0.5, 'P@100': 0.5}\n",
      "Top-Deciles (head):\n",
      "         pos_rate  count      lift\n",
      "decile                           \n",
      "1       0.577778     45  1.070310\n",
      "2       0.288889     45  0.535155\n",
      "3       0.622222     45  1.152641\n"
     ]
    }
   ],
   "source": [
    "# === Vor/Nach: Histogramm + Brier ============================================\n",
    "brier_raw = brier_score_loss(yte, y_test_proba)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(y_test_proba, bins=30, alpha=0.6, label=\"raw\")\n",
    "plt.hist(y_test_cal,  bins=30, alpha=0.6, label=f\"calibrated ({calib_name})\")\n",
    "plt.axvline(thr, linestyle=\"--\", label=f\"thr={thr:.3f}\")\n",
    "plt.title(\"P(y=1) – raw vs. calibrated (Test)\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig(RUN_DIR / \"figures/proba_hist_raw_vs_cal.png\", dpi=160); plt.close()\n",
    "\n",
    "print(f\"Brier raw={brier_raw:.4f} → calibrated={brier:.4f}\")\n",
    "print(\"Precision@k:\", p_at)\n",
    "print(\"Top-Deciles (head):\\n\", deciles.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "200036f8-d7ff-41fe-aa6f-ae6903888710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 4 abgeschlossen.\n",
      "Artefakte:\n",
      " - ..\\results\\2025-10-03_13-48-57_lstm\\preds_test.csv\n",
      " - ..\\results\\2025-10-03_13-48-57_lstm\\evaluation.json\n",
      " - ..\\results\\2025-10-03_13-48-57_lstm\\figures\n"
     ]
    }
   ],
   "source": [
    "# --- Ergebnisse schreiben ---\n",
    "out = {\n",
    "    \"config\": RCFG,\n",
    "    \"features_used\": FEATURES,\n",
    "    \"threshold_selection\": {\n",
    "        \"strategy\": \"max_mcc_with_pos_rate_bounds\",\n",
    "        \"threshold\": float(thr),\n",
    "        \"pos_rate_bounds\": [0.2, 0.8],\n",
    "        \"val_mcc\": float(score_val),\n",
    "    },\n",
    "    \"calibration\": {\"method\": \"isotonic\", \"path\": str(RUN_DIR / \"calibrator.joblib\")},\n",
    "    \"metrics\": {\n",
    "        \"test\": {\n",
    "            \"roc_auc\": float(roc_auc),\n",
    "            \"auprc\": float(ap),\n",
    "            \"brier\": float(brier),\n",
    "            \"balanced_accuracy\": float(bal_acc),\n",
    "            \"mcc\": float(mcc),\n",
    "            \"mcc_bootstrap_ci\": [float(mcc_ci[0]), float(mcc_ci[1]), float(mcc_ci[2])],\n",
    "            \"confusion_matrix\": cm.tolist(),\n",
    "            \"report\": rep,\n",
    "        }\n",
    "    },\n",
    "    \"backtest\": backtest,\n",
    "    \"calibration\": {\n",
    "    \"method\": \"isotonic\",\n",
    "    \"path\": str(RUN_DIR / \"calibrator.joblib\"),\n",
    "    \"brier_raw\": float(brier_raw),\n",
    "    \"brier_cal\": float(brier_cal)\n",
    "},\n",
    "\"diagnostics\": {\n",
    "    \"val_pos_rate_cal\": float(y_val_proba_cal.mean()),\n",
    "    \"test_pos_rate_cal\": float(y_test_proba_cal.mean()),\n",
    "    \"test_pred_pos_rate_at_thr\": float((y_test_proba_cal >= thr).mean()),\n",
    "},\n",
    "\"backtest_conservative\": {\n",
    "    \"final_equity\": float(equity_t1.iloc[-1]),\n",
    "}\n",
    "}\n",
    "with open(RUN_DIR / \"evaluation.json\", \"w\") as f:\n",
    "    json.dump(out, f, indent=2)\n",
    "\n",
    "print(\"\\nBlock 4 abgeschlossen.\")\n",
    "print(\"Artefakte:\")\n",
    "print(\" -\", RUN_DIR / \"preds_test.csv\")\n",
    "print(\" -\", RUN_DIR / \"evaluation.json\")\n",
    "print(\" -\", RUN_DIR / \"figures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c23181-39d6-4d12-b294-a152d62e251b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3852acd-f9d3-495d-b3ff-321f8a6f36c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (finance-lstm)",
   "language": "python",
   "name": "finance-lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
