{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9701daf5-1f64-4226-9ca2-aa40821fc6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Block 5: Walk-Forward Cross-Validation + Hyperparameter-Search ---\n",
    "import os, sys, json, time, logging, glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ROOT = os.path.abspath(\"..\")\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.insert(0, ROOT)\n",
    "\n",
    "# TensorFlow logging ruhigstellen\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db180739-46e5-44c1-a93d-6e99e9c3ff9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WFCV_RUN_DIR: ..\\results\\2025-10-19_16-45-57_wfcv\n"
     ]
    }
   ],
   "source": [
    "# ---- Core-Config laden -------------------------------------------------\n",
    "with open(os.path.join(ROOT, \"config.json\"), \"r\") as f:\n",
    "    C = json.load(f)\n",
    "\n",
    "TICKER, START, END, INTERVAL = C[\"ticker\"], C[\"start\"], C[\"end\"], C[\"interval\"]\n",
    "HORIZON  = int(C[\"horizon\"])\n",
    "LOOKBACK_DEFAULT = int(C[\"lookback\"])\n",
    "BATCH    = int(C.get(\"batch\", 64))\n",
    "SEED     = int(C.get(\"seed\", 42))\n",
    "FEATURESET = C.get(\"featureset\", \"v2\")\n",
    "EPS_MODE   = C.get(\"epsilon_mode\", \"abs\")\n",
    "EPSILON    = float(C.get(\"epsilon\", 0.0005))\n",
    "RESULTS_DIR = Path(C.get(\"results_dir\", \"../results\"))\n",
    "\n",
    "np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "# WFCV Run-Ordner\n",
    "RUN_DIR = RESULTS_DIR / time.strftime(\"%Y-%m-%d_%H-%M-%S_wfcv\")\n",
    "(RUN_DIR / \"plots\").mkdir(parents=True, exist_ok=True)\n",
    "print(\"WFCV_RUN_DIR:\", RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c55da04a-e118-4fc9-a5f2-339a4a1fbcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Optional: schneller Smoke-Test -----------------------------------\n",
    "FAST = False            # <- nur auf True setzen, wenn du bewusst 3 Folds & kurze Runs willst\n",
    "EPOCHS_GRID = 60        # Block-5-EPOCHS (unabhängig von Block 3)\n",
    "N_FOLDS = 5             # Ziel: wirklich 5 Folds\n",
    "if FAST:\n",
    "    EPOCHS_GRID = 25\n",
    "    N_FOLDS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b50b8c3e-f514-45e3-97c6-3f649cd47ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TRAIN_CSV: ../data/AAPL_1d_2012-01-01_2025-09-01_cls_h5_abs0p0005.csv\n",
      "Label pos_rate: 0.563 | n: 3402\n"
     ]
    }
   ],
   "source": [
    "# ---- Daten & Features --------------------------------------------------\n",
    "import yaml, re, glob, os\n",
    "\n",
    "yaml_path = f\"../data/features_{FEATURESET}.yml\"\n",
    "meta = {}\n",
    "label_h = label_mode = label_eps = None\n",
    "\n",
    "if os.path.exists(yaml_path):\n",
    "    with open(yaml_path, \"r\") as f:\n",
    "        meta = yaml.safe_load(f) or {}\n",
    "    lab = (meta or {}).get(\"label\", {})\n",
    "    label_h    = lab.get(\"horizon\", None)\n",
    "    label_mode = lab.get(\"mode\", None)\n",
    "    label_eps  = lab.get(\"epsilon\", None)\n",
    "\n",
    "def _parse_h_meps_from_name(path: str):\n",
    "    mH = re.search(r\"_cls_h(\\d+)_\", path)\n",
    "    me = re.search(r\"_(abs|rel)(\\d+p\\d+)\\.csv$\", path)\n",
    "    H  = int(mH.group(1)) if mH else None\n",
    "    md = me.group(1) if me else None\n",
    "    eps= float(me.group(2).replace(\"p\",\".\")) if me else None\n",
    "    return H, md, eps\n",
    "\n",
    "def _infer_from_existing_files(tkr, itv, start, end, mode_hint=None, eps_hint=None):\n",
    "    pat = f\"../data/{tkr}_{itv}_{start}_{end}_cls_h*_.csv\".replace(\"_ .csv\",\".csv\")\n",
    "    cands = sorted(glob.glob(pat), key=os.path.getmtime)\n",
    "    # Wenn Mode/Eps bekannt, einschränken\n",
    "    if mode_hint and (eps_hint is not None):\n",
    "        tag = f\"{mode_hint}{str(eps_hint).replace('.','p')}\"\n",
    "        cands = [c for c in cands if c.endswith(f\"_{tag}.csv\")]\n",
    "    if not cands:\n",
    "        return None\n",
    "    return _parse_h_meps_from_name(cands[-1])\n",
    "\n",
    "# 1) Primär: YAML-Wahrheit\n",
    "H_FOR_FILE    = int(label_h)    if label_h    is not None else None\n",
    "MODE_FOR_FILE = str(label_mode) if label_mode is not None else None\n",
    "EPS_FOR_FILE  = float(label_eps) if label_eps is not None else None\n",
    "\n",
    "# 2) Sonst: aus vorhandenen CSVs ableiten (ggf. mit Mode/Eps-Hinweis aus YAML)\n",
    "if (H_FOR_FILE is None) or (MODE_FOR_FILE is None) or (EPS_FOR_FILE is None):\n",
    "    inferred = _infer_from_existing_files(TICKER, INTERVAL, START, END,\n",
    "                                          mode_hint=MODE_FOR_FILE, eps_hint=EPS_FOR_FILE)\n",
    "    if inferred is not None:\n",
    "        H_i, M_i, E_i = inferred\n",
    "        H_FOR_FILE    = H_FOR_FILE    if H_FOR_FILE    is not None else H_i\n",
    "        MODE_FOR_FILE = MODE_FOR_FILE if MODE_FOR_FILE is not None else M_i\n",
    "        EPS_FOR_FILE  = EPS_FOR_FILE  if EPS_FOR_FILE  is not None else E_i\n",
    "\n",
    "# 3) Wenn danach immer noch unbestimmt → explizit scheitern (Anwender-Hinweis)\n",
    "if (H_FOR_FILE is None) or (MODE_FOR_FILE is None) or (EPS_FOR_FILE is None):\n",
    "    raise RuntimeError(\n",
    "        \"Label-Definition (H/mode/epsilon) konnte nicht aus YAML oder existierenden CSV-Dateien bestimmt werden.\\n\"\n",
    "        f\"Erwartet YAML unter: {yaml_path}\\n\"\n",
    "        \"Oder eine Datei wie: ../data/\"\n",
    "        f\"{TICKER}_{INTERVAL}_{START}_{END}_cls_h<H>_<abs|rel><epsilon_mit_p>.csv\"\n",
    "    )\n",
    "\n",
    "eps_tag   = f\"{MODE_FOR_FILE}{str(EPS_FOR_FILE).replace('.','p')}\"\n",
    "TRAIN_CSV = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h{H_FOR_FILE}_{eps_tag}.csv\"\n",
    "\n",
    "if not os.path.exists(TRAIN_CSV):\n",
    "    pat = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h*_{eps_tag}.csv\"\n",
    "    candidates = sorted(glob.glob(pat), key=os.path.getmtime)\n",
    "    if candidates:\n",
    "        TRAIN_CSV = candidates[-1]\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Train CSV nicht gefunden: {TRAIN_CSV}\\n\"\n",
    "            f\"Gesucht nach Pattern: {pat}\\n\"\n",
    "            \"Hinweis: Block 2 mit dieser Label-Definition laufen lassen.\"\n",
    "        )\n",
    "\n",
    "print(\"Loaded TRAIN_CSV:\", TRAIN_CSV)\n",
    "df = pd.read_csv(TRAIN_CSV, index_col=0, parse_dates=True).sort_index()\n",
    "\n",
    "OHLCV = {\"open\",\"high\",\"low\",\"close\",\"volume\"}\n",
    "if meta:\n",
    "    FEATURES_ALL = [c for c in meta.get(\"features\", []) if c in df.columns]\n",
    "else:\n",
    "    FEATURES_ALL = [c for c in df.columns if c not in (OHLCV | {\"target\"})]\n",
    "assert len(FEATURES_ALL) > 0, \"Keine Features gefunden.\"\n",
    "\n",
    "X_full = df[FEATURES_ALL].copy()\n",
    "y_full = df[\"target\"].astype(int).copy()\n",
    "\n",
    "print(\"Label pos_rate:\", round(y_full.mean(), 3), \"| n:\", len(y_full))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "408b2564-2d68-4706-838e-f3e1a499c7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Folds: 5\n"
     ]
    }
   ],
   "source": [
    "# ---- Walk-Forward Splits (fix: 5 Folds, val=20%, min_train=45%) -------\n",
    "def make_wf_splits(n, n_folds=5, val_frac=0.20, min_train_frac=0.45):\n",
    "    \"\"\"\n",
    "    Rolling WF:\n",
    "      train = [0 : train_end)\n",
    "      val   = [train_end : val_end)\n",
    "    Erzeugt *genau* n_folds Fenster, gleichmäßig über den Restbereich verteilt.\n",
    "    \"\"\"\n",
    "    val_len   = max(60, int(round(n * val_frac)))             # min ca. 3 Monate\n",
    "    min_train = max(200, int(round(n * min_train_frac)))\n",
    "    start_val_end = min_train + val_len\n",
    "    if start_val_end + 1 > n:\n",
    "        raise ValueError(f\"Dataset zu kurz für val_frac/min_train_frac: n={n}, \"\n",
    "                         f\"min_train={min_train}, val_len={val_len}\")\n",
    "\n",
    "    # Val-Enden gleichmäßig verteilen (inkl. letzter Punkt nahe n)\n",
    "    val_ends = np.linspace(start_val_end, n, num=n_folds, endpoint=True).astype(int)\n",
    "\n",
    "    # In seltenen Fällen können nahe beieinander liegende val_ends Duplikate erzeugen -> deduplizieren\n",
    "    val_ends = np.unique(val_ends)\n",
    "    if len(val_ends) < n_folds:\n",
    "        # Fallback: mit Schrittweite erhöhen, bis wir n_folds erhalten\n",
    "        step = max(1, (n - start_val_end) // n_folds)\n",
    "        val_ends = np.arange(start_val_end, start_val_end + step * n_folds, step)\n",
    "        val_ends = np.clip(val_ends, start_val_end, n)\n",
    "\n",
    "    stops = []\n",
    "    for ve in val_ends[:n_folds]:\n",
    "        te = int(ve - val_len)\n",
    "        te = max(te, LOOKBACK_DEFAULT + 1)   # genug für Windowing\n",
    "        if te <= 0 or ve <= te or ve > n:\n",
    "            continue\n",
    "        stops.append((slice(0, te), slice(te, ve)))\n",
    "    if len(stops) != n_folds:\n",
    "        raise RuntimeError(f\"Erzeugte nur {len(stops)} von {n_folds} Folds. \"\n",
    "                           f\"Bitte val_frac/min_train_frac prüfen.\")\n",
    "    return stops\n",
    "\n",
    "n = len(df)\n",
    "splits = make_wf_splits(n, n_folds=N_FOLDS, val_frac=0.20, min_train_frac=0.45)\n",
    "print(\"Anzahl Folds:\", len(splits))\n",
    "if len(splits) != N_FOLDS:\n",
    "    raise RuntimeError(\"Es sind nicht exakt die gewünschten Folds entstanden.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7faef79f-6bf2-4968-b014-0638ef49ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Hilfsfunktionen: Windowing + Pipeline -----------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef, average_precision_score, roc_auc_score\n",
    "from tensorflow.keras import layers, regularizers, callbacks, optimizers, models\n",
    "\n",
    "def make_windows(X_df, y_ser, lookback):\n",
    "    Xv = X_df.values.astype(np.float32)\n",
    "    yv = y_ser.values.astype(np.int32)\n",
    "    xs, ys = [], []\n",
    "    for i in range(lookback-1, len(X_df)):\n",
    "        xs.append(Xv[i - lookback + 1 : i + 1])\n",
    "        ys.append(yv[i])\n",
    "    return np.stack(xs, axis=0), np.array(ys)\n",
    "\n",
    "def build_model(n_features, width1=64, width2=32, dropout=0.10, lr=5e-4, use_gru=True):\n",
    "    rnn = layers.GRU if use_gru else layers.LSTM\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=(None, n_features)),\n",
    "        rnn(width1, return_sequences=True, recurrent_dropout=dropout),\n",
    "        layers.LayerNormalization(),\n",
    "        rnn(width2, recurrent_dropout=dropout),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.Dense(16, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-5)),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ])\n",
    "    m.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.AUC(name=\"auc\"),\n",
    "                 tf.keras.metrics.AUC(name=\"auprc\", curve=\"PR\")]\n",
    "    )\n",
    "    return m\n",
    "\n",
    "def fit_eval_fold(X_tr, y_tr, X_va, y_va, lookback, hp, seed=SEED, batch=BATCH, epochs=EPOCHS_GRID):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Scaler nur auf Train\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_s = pd.DataFrame(scaler.fit_transform(X_tr), index=X_tr.index, columns=X_tr.columns)\n",
    "    X_va_s = pd.DataFrame(scaler.transform(X_va),     index=X_va.index, columns=X_va.columns)\n",
    "\n",
    "    Xtr_win, ytr = make_windows(X_tr_s, y_tr, lookback)\n",
    "    Xva_win, yva = make_windows(X_va_s, y_va, lookback)\n",
    "\n",
    "    ds_tr = tf.data.Dataset.from_tensor_slices((Xtr_win, ytr)) \\\n",
    "            .shuffle(len(Xtr_win), seed=seed) \\\n",
    "            .batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    ds_va = tf.data.Dataset.from_tensor_slices((Xva_win, yva)) \\\n",
    "            .batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    model = build_model(\n",
    "        n_features=Xtr_win.shape[-1],\n",
    "        width1=hp[\"width1\"], width2=hp[\"width2\"],\n",
    "        dropout=hp[\"dropout\"], lr=hp[\"lr\"], use_gru=(hp[\"cell\"]==\"GRU\")\n",
    "    )\n",
    "\n",
    "    cbs = [\n",
    "        callbacks.EarlyStopping(monitor=\"val_auprc\", mode=\"max\", patience=6, restore_best_weights=True),\n",
    "        callbacks.ReduceLROnPlateau(monitor=\"val_auprc\", mode=\"max\", factor=0.5, patience=3, min_lr=1e-5),\n",
    "        callbacks.TerminateOnNaN(),\n",
    "    ]\n",
    "\n",
    "    print(f\"  -> fit (epochs={epochs}, batch={batch}) ...\", flush=True)\n",
    "    hist = model.fit(ds_tr, validation_data=ds_va, epochs=epochs, verbose=1 if FAST else 0, callbacks=cbs)\n",
    "\n",
    "    yva_proba = model.predict(ds_va, verbose=0).ravel()\n",
    "    yva_pred  = (yva_proba >= 0.5).astype(int)\n",
    "\n",
    "    metrics = dict(\n",
    "        mcc=float(matthews_corrcoef(yva, yva_pred)),\n",
    "        bal_acc=float(balanced_accuracy_score(yva, yva_pred)),\n",
    "        auprc=float(average_precision_score(yva, yva_proba)),\n",
    "        auroc=float(roc_auc_score(yva, yva_proba)),\n",
    "        epochs_trained=int(len(hist.history[\"loss\"]))\n",
    "    )\n",
    "    tf.keras.backend.clear_session()\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "337cbd2c-95af-41f5-9be0-dd323baf6fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check vor der Suche:\n",
      "  n Zeilen df: 3402\n",
      "  FEATURESET: v2\n",
      "  FEATURES_ALL: 11\n",
      "  FEATURE_SUBSETS: ['all', 'mom_only']\n",
      "  LOOKBACK_GRID: [60]\n",
      "  HP_GRID: 1\n",
      "  #splits: 5\n",
      "  Fold0 sizes (train/val): 1531 680\n"
     ]
    }
   ],
   "source": [
    "# --- Grids --------------------------------------------------------------\n",
    "LOOKBACK_GRID = [LOOKBACK_DEFAULT] if FAST else [LOOKBACK_DEFAULT]  # kannst hier weitere LB adden\n",
    "HP_GRID = [\n",
    "    dict(width1=32, width2=16, dropout=0.10, lr=5e-4, cell=\"GRU\"),\n",
    "] if FAST else [\n",
    "    dict(width1=32, width2=16, dropout=0.10, lr=5e-4, cell=\"GRU\"),\n",
    "    # weitere Kandidaten optional:\n",
    "    # dict(width1=64, width2=32, dropout=0.10, lr=5e-4, cell=\"LSTM\"),\n",
    "]\n",
    "\n",
    "FEATURE_SUBSETS = {\n",
    "    \"all\": FEATURES_ALL,\n",
    "    \"mom_only\": [c for c in FEATURES_ALL\n",
    "                 if (\"logret\" in c) or (\"macd\" in c) or (c in {\"sma_diff\",\"rsi_14\",\"bb_pos\"})],\n",
    "}\n",
    "\n",
    "print(\"Sanity check vor der Suche:\")\n",
    "print(\"  n Zeilen df:\", len(df))\n",
    "print(\"  FEATURESET:\", FEATURESET)\n",
    "print(\"  FEATURES_ALL:\", len(FEATURES_ALL))\n",
    "print(\"  FEATURE_SUBSETS:\", list(FEATURE_SUBSETS.keys()))\n",
    "print(\"  LOOKBACK_GRID:\", LOOKBACK_GRID)\n",
    "print(\"  HP_GRID:\", len(HP_GRID))\n",
    "print(\"  #splits:\", len(splits))\n",
    "if len(splits) > 0:\n",
    "    tr_s, va_s = splits[0]\n",
    "    print(\"  Fold0 sizes (train/val):\", tr_s.stop, va_s.stop - va_s.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42408c93-f83a-4abc-ab98-1d0279037343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Suche ...\n",
      "  -> fit (epochs=60, batch=64) ...\n",
      "[all | LB=60 | GRU 32/16 dp=0.1 lr=0.0005] Fold1: MCC=-0.086 AUPRC=0.625\n",
      "  -> fit (epochs=60, batch=64) ...\n",
      "[all | LB=60 | GRU 32/16 dp=0.1 lr=0.0005] Fold2: MCC=-0.031 AUPRC=0.611\n",
      "  -> fit (epochs=60, batch=64) ...\n",
      "[all | LB=60 | GRU 32/16 dp=0.1 lr=0.0005] Fold3: MCC=0.074 AUPRC=0.535\n",
      "  -> fit (epochs=60, batch=64) ...\n",
      "[all | LB=60 | GRU 32/16 dp=0.1 lr=0.0005] Fold4: MCC=0.024 AUPRC=0.563\n",
      "  -> fit (epochs=60, batch=64) ...\n",
      "[all | LB=60 | GRU 32/16 dp=0.1 lr=0.0005] Fold5: MCC=-0.037 AUPRC=0.524\n",
      "--> Summary [all | LB=60 | GRU 32/16] ran_folds=5/5\n",
      "  -> fit (epochs=60, batch=64) ...\n",
      "[mom_only | LB=60 | GRU 32/16 dp=0.1 lr=0.0005] Fold1: MCC=0.022 AUPRC=0.673\n",
      "  -> fit (epochs=60, batch=64) ...\n",
      "[mom_only | LB=60 | GRU 32/16 dp=0.1 lr=0.0005] Fold2: MCC=0.073 AUPRC=0.653\n",
      "  -> fit (epochs=60, batch=64) ...\n",
      "[mom_only | LB=60 | GRU 32/16 dp=0.1 lr=0.0005] Fold3: MCC=0.107 AUPRC=0.538\n",
      "  -> fit (epochs=60, batch=64) ...\n",
      "[mom_only | LB=60 | GRU 32/16 dp=0.1 lr=0.0005] Fold4: MCC=0.016 AUPRC=0.587\n",
      "  -> fit (epochs=60, batch=64) ...\n",
      "[mom_only | LB=60 | GRU 32/16 dp=0.1 lr=0.0005] Fold5: MCC=0.007 AUPRC=0.608\n",
      "--> Summary [mom_only | LB=60 | GRU 32/16] ran_folds=5/5\n",
      "\n",
      "Suche fertig: combos=2, records=10, Dauer=112.1s\n",
      "Geschrieben: ..\\results\\2025-10-19_16-45-57_wfcv\\wfcv_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ---- Suche --------------------------------------------------------------\n",
    "from time import perf_counter\n",
    "print(\"Starte Suche ...\", flush=True)\n",
    "records = []\n",
    "total_combos = 0\n",
    "\n",
    "def _size(slc):  # helper\n",
    "    return slc.stop - (slc.start or 0)\n",
    "\n",
    "t0 = perf_counter()\n",
    "for feat_name, FEATS in FEATURE_SUBSETS.items():\n",
    "    if len(FEATS) == 0:\n",
    "        print(f\"[{feat_name}] übersprungen: 0 Features.\")\n",
    "        continue\n",
    "\n",
    "    for lookback in LOOKBACK_GRID:\n",
    "        for hp in HP_GRID:\n",
    "            total_combos += 1\n",
    "            ran_folds = 0\n",
    "            for fold_id, (tr_s, va_s) in enumerate(splits, start=1):\n",
    "                X_tr, y_tr = X_full.iloc[tr_s][FEATS], y_full.iloc[tr_s]\n",
    "                X_va, y_va = X_full.iloc[va_s][FEATS], y_full.iloc[va_s]\n",
    "\n",
    "                # Minimalcheck für Windowing\n",
    "                if len(X_tr) < (lookback + 50) or len(X_va) < (lookback + 10):\n",
    "                    print(f\"[{feat_name} | LB={lookback} | {hp['cell']} {hp['width1']}/{hp['width2']} \"\n",
    "                          f\"dp={hp['dropout']} lr={hp['lr']}] Skip-Fold (zu kurz): \"\n",
    "                          f\"train={len(X_tr)}, val={len(X_va)}, needed>={(lookback+50)}/{(lookback+10)}\")\n",
    "                    continue\n",
    "\n",
    "                mets = fit_eval_fold(X_tr, y_tr, X_va, y_va, lookback, hp, epochs=EPOCHS_GRID)\n",
    "                ran_folds += 1\n",
    "\n",
    "                rec = {\n",
    "                    \"feature_set\": FEATURESET,\n",
    "                    \"features_used\": feat_name,\n",
    "                    \"n_features\": len(FEATS),\n",
    "                    \"lookback\": lookback,\n",
    "                    **hp,\n",
    "                    \"fold\": fold_id,\n",
    "                    **mets\n",
    "                }\n",
    "                records.append(rec)\n",
    "                print(f\"[{feat_name} | LB={lookback} | {hp['cell']} {hp['width1']}/{hp['width2']} \"\n",
    "                      f\"dp={hp['dropout']} lr={hp['lr']}] Fold{fold_id}: \"\n",
    "                      f\"MCC={mets['mcc']:.3f} AUPRC={mets['auprc']:.3f}\")\n",
    "\n",
    "            print(f\"--> Summary [{feat_name} | LB={lookback} | {hp['cell']} {hp['width1']}/{hp['width2']}] \"\n",
    "                  f\"ran_folds={ran_folds}/{len(splits)}\")\n",
    "\n",
    "t1 = perf_counter()\n",
    "print(f\"\\nSuche fertig: combos={total_combos}, records={len(records)}, Dauer={t1-t0:.1f}s\")\n",
    "\n",
    "results = pd.DataFrame.from_records(records)\n",
    "csv_path = RUN_DIR / \"wfcv_results.csv\"\n",
    "results.to_csv(csv_path, index=False)\n",
    "print(\"Geschrieben:\", csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2afca6e0-5c56-45c9-93a0-066fc19b613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config: {'feature_set': 'v2', 'features_used': 'mom_only', 'n_features': 9, 'lookback': 60, 'width1': 32, 'width2': 16, 'dropout': 0.1, 'lr': 0.0005, 'cell': 'GRU', 'mcc_mean': 0.0452054373880242, 'mcc_std': 0.04308407858833591, 'auprc_mean': 0.6119489902015658, 'auprc_std': 0.053643328854503874, 'auroc_mean': 0.5534801696414117, 'balacc_mean': 0.5103286957052815, 'n_folds': 5}\n"
     ]
    }
   ],
   "source": [
    "# ---- Aggregation & Best-Config -----------------------------------------\n",
    "agg_cols = [\"feature_set\",\"features_used\",\"n_features\",\"lookback\",\"width1\",\"width2\",\"dropout\",\"lr\",\"cell\"]\n",
    "agg = (results.groupby(agg_cols)\n",
    "       .agg(mcc_mean=(\"mcc\",\"mean\"), mcc_std=(\"mcc\",\"std\"),\n",
    "            auprc_mean=(\"auprc\",\"mean\"), auprc_std=(\"auprc\",\"std\"),\n",
    "            auroc_mean=(\"auroc\",\"mean\"),\n",
    "            balacc_mean=(\"bal_acc\",\"mean\"),\n",
    "            n_folds=(\"mcc\",\"count\"))\n",
    "       .reset_index())\n",
    "\n",
    "# Ranking: mcc_mean (desc) -> auprc_mean (desc) -> mcc_std (asc)\n",
    "agg = agg.sort_values([\"mcc_mean\",\"auprc_mean\",\"mcc_std\"], ascending=[False, False, True])\n",
    "agg.to_csv(RUN_DIR / \"wfcv_results_agg.csv\", index=False)\n",
    "\n",
    "best = agg.iloc[0].to_dict() if len(agg) else {}\n",
    "with open(RUN_DIR / \"best_config.json\", \"w\") as f:\n",
    "    json.dump(best, f, indent=2)\n",
    "\n",
    "# (Optional) Top-5\n",
    "agg.head(5).to_csv(RUN_DIR / \"wfcv_results_top5.csv\", index=False)\n",
    "\n",
    "print(\"Best config:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a79f67d2-a756-442f-8aaa-047113714208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Score-Grids als Plot ----------------------------------------------\n",
    "pivot_mcc = agg.pivot_table(index=\"lookback\",\n",
    "                            columns=[\"features_used\", \"cell\", \"width1\"],\n",
    "                            values=\"mcc_mean\")\n",
    "pivot_au  = agg.pivot_table(index=\"lookback\",\n",
    "                            columns=[\"features_used\", \"cell\", \"width1\"],\n",
    "                            values=\"auprc_mean\")\n",
    "\n",
    "for name, pivot in [(\"score_grid_mcc.png\", pivot_mcc), (\"score_grid_auprc.png\", pivot_au)]:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    im = plt.imshow(pivot.values, aspect=\"auto\")\n",
    "    plt.colorbar(im)\n",
    "    plt.yticks(range(len(pivot.index)), pivot.index)\n",
    "    plt.xticks(range(pivot.shape[1]), [str(c) for c in pivot.columns], rotation=45, ha=\"right\")\n",
    "    plt.title(name.replace(\"_\",\" \").replace(\".png\",\"\"))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RUN_DIR / \"plots\" / name, dpi=160)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5581e69e-5b28-4224-9026-2872d2313683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 5 abgeschlossen. Artefakte:\n",
      " - ..\\results\\2025-10-19_16-45-57_wfcv\\wfcv_results.csv\n",
      " - ..\\results\\2025-10-19_16-45-57_wfcv\\wfcv_results_agg.csv\n",
      " - ..\\results\\2025-10-19_16-45-57_wfcv\\best_config.json\n",
      " - ..\\results\\2025-10-19_16-45-57_wfcv\\wfcv_results_top5.csv\n",
      " - ..\\results\\2025-10-19_16-45-57_wfcv\\plots\n"
     ]
    }
   ],
   "source": [
    "# ---- Run-Info dump ------------------------------------------------------\n",
    "run_info = {\n",
    "    \"seed\": SEED,\n",
    "    \"epochs_grid\": EPOCHS_GRID,\n",
    "    \"n_folds\": N_FOLDS,\n",
    "    \"val_frac\": 0.20,\n",
    "    \"min_train_frac\": 0.45,\n",
    "    \"lookback_grid\": LOOKBACK_GRID,\n",
    "    \"hp_grid\": HP_GRID,\n",
    "    \"feature_subsets\": list(FEATURE_SUBSETS.keys()),\n",
    "    \"train_csv\": TRAIN_CSV,\n",
    "    \"label_resolution\": {\n",
    "        \"source\": \"yaml\" if os.path.exists(yaml_path) and (label_h is not None) else \"inferred_from_csv\",\n",
    "        \"yaml_path\": yaml_path\n",
    "    },\n",
    "    \"labels\": {\"horizon\": H_FOR_FILE, \"mode\": MODE_FOR_FILE, \"epsilon\": EPS_FOR_FILE}\n",
    "}\n",
    "with open(RUN_DIR / \"wfcv_run_info.json\", \"w\") as f:\n",
    "    json.dump(run_info, f, indent=2)\n",
    "\n",
    "print(\"\\nBlock 5 abgeschlossen. Artefakte:\")\n",
    "print(\" -\", RUN_DIR / \"wfcv_results.csv\")\n",
    "print(\" -\", RUN_DIR / \"wfcv_results_agg.csv\")\n",
    "print(\" -\", RUN_DIR / \"best_config.json\")\n",
    "print(\" -\", RUN_DIR / \"wfcv_results_top5.csv\")\n",
    "print(\" -\", RUN_DIR / \"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6703011-38bf-428d-b1b5-a9537e1903d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (finance-lstm)",
   "language": "python",
   "name": "finance-lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
