{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9701daf5-1f64-4226-9ca2-aa40821fc6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Block 5: Walk-Forward Cross-Validation + Hyperparameter-Search ---\n",
    "import os, sys, json, time, logging, glob, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ROOT = os.path.abspath(\"..\")\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.insert(0, ROOT)\n",
    "\n",
    "# TensorFlow logging ruhigstellen\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db180739-46e5-44c1-a93d-6e99e9c3ff9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WFCV_RUN_DIR: ..\\results\\2025-10-22_12-13-21_wfcv\n"
     ]
    }
   ],
   "source": [
    "# ---- Core-Config laden -------------------------------------------------\n",
    "with open(os.path.join(ROOT, \"config.json\"), \"r\") as f:\n",
    "    C = json.load(f)\n",
    "\n",
    "TICKER, START, END, INTERVAL = C[\"ticker\"], C[\"start\"], C[\"end\"], C[\"interval\"]\n",
    "HORIZON  = int(C[\"horizon\"])\n",
    "LOOKBACK_DEFAULT = int(C[\"lookback\"])\n",
    "BATCH    = int(C.get(\"batch\", 64))\n",
    "SEED     = int(C.get(\"seed\", 42))\n",
    "FEATURESET = C.get(\"featureset\", \"v2\")\n",
    "EPS_MODE   = C.get(\"epsilon_mode\", \"abs\")\n",
    "EPSILON    = float(C.get(\"epsilon\", 0.0005))\n",
    "RESULTS_DIR = Path(C.get(\"results_dir\", \"../results\"))\n",
    "\n",
    "np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "# WFCV Run-Ordner\n",
    "RUN_DIR = RESULTS_DIR / time.strftime(\"%Y-%m-%d_%H-%M-%S_wfcv\")\n",
    "(RUN_DIR / \"plots\").mkdir(parents=True, exist_ok=True)\n",
    "print(\"WFCV_RUN_DIR:\", RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55da04a-e118-4fc9-a5f2-339a4a1fbcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Optional: schneller Smoke-Test -----------------------------------\n",
    "FAST = C.get(\"fast_wfcv\", False)  # CHANGE: auch aus config.json aktivierbar\n",
    "EPOCHS_GRID = 60\n",
    "N_FOLDS = 5\n",
    "if FAST:\n",
    "    EPOCHS_GRID = 25\n",
    "    N_FOLDS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50b8c3e-f514-45e3-97c6-3f649cd47ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TRAIN_CSV: ../data/AAPL_1d_2012-01-01_2025-09-01_cls_h1_abs0p0005.csv\n",
      "Label pos_rate: 0.509 | n: 3402\n"
     ]
    }
   ],
   "source": [
    "# ---- Daten & Features --------------------------------------------------\n",
    "import yaml\n",
    "\n",
    "yaml_path = f\"../data/features_{FEATURESET}.yml\"\n",
    "meta = {}\n",
    "label_h = label_mode = label_eps = None\n",
    "\n",
    "if os.path.exists(yaml_path):\n",
    "    with open(yaml_path, \"r\") as f:\n",
    "        meta = yaml.safe_load(f) or {}\n",
    "    lab = (meta or {}).get(\"label\", {})\n",
    "    label_h    = lab.get(\"horizon\", None)\n",
    "    label_mode = lab.get(\"mode\", None)\n",
    "    label_eps  = lab.get(\"epsilon\", None)\n",
    "\n",
    "def _parse_h_meps_from_name(path: str):\n",
    "    mH = re.search(r\"_cls_h(\\d+)_\", path)\n",
    "    me = re.search(r\"_(abs|rel)(\\d+p\\d+)\\.csv$\", path)\n",
    "    H  = int(mH.group(1)) if mH else None\n",
    "    md = me.group(1) if me else None\n",
    "    eps= float(me.group(2).replace(\"p\",\".\")) if me else None\n",
    "    return H, md, eps\n",
    "\n",
    "def _infer_from_existing_files(tkr, itv, start, end, mode_hint=None, eps_hint=None):\n",
    "    pat = f\"../data/{tkr}_{itv}_{start}_{end}_cls_h*_.csv\".replace(\"_ .csv\",\".csv\")\n",
    "    cands = sorted(glob.glob(pat), key=os.path.getmtime)\n",
    "    if mode_hint and (eps_hint is not None):\n",
    "        tag = f\"{mode_hint}{str(eps_hint).replace('.','p')}\"\n",
    "        cands = [c for c in cands if c.endswith(f\"_{tag}.csv\")]\n",
    "    if not cands:\n",
    "        return None\n",
    "    return _parse_h_meps_from_name(cands[-1])\n",
    "\n",
    "H_FOR_FILE    = int(label_h)    if label_h    is not None else None\n",
    "MODE_FOR_FILE = str(label_mode) if label_mode is not None else None\n",
    "EPS_FOR_FILE  = float(label_eps) if label_eps is not None else None\n",
    "\n",
    "if (H_FOR_FILE is None) or (MODE_FOR_FILE is None) or (EPS_FOR_FILE is None):\n",
    "    inferred = _infer_from_existing_files(TICKER, INTERVAL, START, END,\n",
    "                                          mode_hint=MODE_FOR_FILE, eps_hint=EPS_FOR_FILE)\n",
    "    if inferred is not None:\n",
    "        H_i, M_i, E_i = inferred\n",
    "        H_FOR_FILE    = H_FOR_FILE    if H_FOR_FILE    is not None else H_i\n",
    "        MODE_FOR_FILE = MODE_FOR_FILE if MODE_FOR_FILE is not None else M_i\n",
    "        EPS_FOR_FILE  = EPS_FOR_FILE  if EPS_FOR_FILE  is not None else E_i\n",
    "\n",
    "if (H_FOR_FILE is None) or (MODE_FOR_FILE is None) or (EPS_FOR_FILE is None):\n",
    "    raise RuntimeError(\n",
    "        \"Label-Definition (H/mode/epsilon) konnte nicht aus YAML oder existierenden CSV-Dateien bestimmt werden.\\n\"\n",
    "        f\"Erwartet YAML unter: {yaml_path}\\n\"\n",
    "        \"Oder eine Datei wie: ../data/\"\n",
    "        f\"{TICKER}_{INTERVAL}_{START}_{END}_cls_h<H>_<abs|rel><epsilon_mit_p>.csv\"\n",
    "    )\n",
    "\n",
    "eps_tag   = f\"{MODE_FOR_FILE}{str(EPS_FOR_FILE).replace('.','p')}\"\n",
    "TRAIN_CSV = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h{H_FOR_FILE}_{eps_tag}.csv\"\n",
    "\n",
    "if not os.path.exists(TRAIN_CSV):\n",
    "    pat = f\"../data/{TICKER}_{INTERVAL}_{START}_{END}_cls_h*_{eps_tag}.csv\"\n",
    "    candidates = sorted(glob.glob(pat), key=os.path.getmtime)\n",
    "    if candidates:\n",
    "        TRAIN_CSV = candidates[-1]\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Train CSV nicht gefunden: {TRAIN_CSV}\\n\"\n",
    "            f\"Gesucht nach Pattern: {pat}\\n\"\n",
    "            \"Hinweis: Block 2 mit dieser Label-Definition laufen lassen.\"\n",
    "        )\n",
    "\n",
    "print(\"Loaded TRAIN_CSV:\", TRAIN_CSV)\n",
    "df = pd.read_csv(TRAIN_CSV, index_col=0, parse_dates=True).sort_index()\n",
    "\n",
    "OHLCV = {\"open\",\"high\",\"low\",\"close\",\"volume\"}\n",
    "if meta:\n",
    "    FEATURES_ALL = [c for c in meta.get(\"features\", []) if c in df.columns]\n",
    "else:\n",
    "    FEATURES_ALL = [c for c in df.columns if c not in (OHLCV | {\"target\"})]\n",
    "assert len(FEATURES_ALL) > 0, \"Keine Features gefunden.\"\n",
    "\n",
    "X_full = df[FEATURES_ALL].copy()\n",
    "y_full = df[\"target\"].astype(int).copy()\n",
    "\n",
    "print(\"Label pos_rate:\", round(y_full.mean(), 3), \"| n:\", len(y_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "408b2564-2d68-4706-838e-f3e1a499c7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Folds: 5\n"
     ]
    }
   ],
   "source": [
    "# ---- Walk-Forward Splits (fix: 5 Folds, val=20%, min_train=45%) -------\n",
    "def make_wf_splits(n, n_folds=5, val_frac=0.20, min_train_frac=0.45):\n",
    "    val_len   = max(60, int(round(n * val_frac)))\n",
    "    min_train = max(200, int(round(n * min_train_frac)))\n",
    "    start_val_end = min_train + val_len\n",
    "    if start_val_end + 1 > n:\n",
    "        raise ValueError(f\"Dataset zu kurz für val_frac/min_train_frac: n={n}, \"\n",
    "                         f\"min_train={min_train}, val_len={val_len}\")\n",
    "    val_ends = np.linspace(start_val_end, n, num=n_folds, endpoint=True).astype(int)\n",
    "    val_ends = np.unique(val_ends)\n",
    "    if len(val_ends) < n_folds:\n",
    "        step = max(1, (n - start_val_end) // n_folds)\n",
    "        val_ends = np.arange(start_val_end, start_val_end + step * n_folds, step)\n",
    "        val_ends = np.clip(val_ends, start_val_end, n)\n",
    "    stops = []\n",
    "    for ve in val_ends[:n_folds]:\n",
    "        te = int(ve - val_len)\n",
    "        te = max(te, LOOKBACK_DEFAULT + 1)\n",
    "        if te <= 0 or ve <= te or ve > n:\n",
    "            continue\n",
    "        stops.append((slice(0, te), slice(te, ve)))\n",
    "    if len(stops) != n_folds:\n",
    "        raise RuntimeError(f\"Erzeugte nur {len(stops)} von {n_folds} Folds.\")\n",
    "    return stops\n",
    "\n",
    "n = len(df)\n",
    "splits = make_wf_splits(n, n_folds=N_FOLDS, val_frac=0.20, min_train_frac=0.45)\n",
    "print(\"Anzahl Folds:\", len(splits))\n",
    "if len(splits) != N_FOLDS:\n",
    "    raise RuntimeError(\"Es sind nicht exakt die gewünschten Folds entstanden.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7faef79f-6bf2-4968-b014-0638ef49ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Hilfsfunktionen: Windowing + Pipeline -----------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef, average_precision_score, roc_auc_score\n",
    "from tensorflow.keras import layers, regularizers, callbacks, optimizers, models\n",
    "\n",
    "def make_windows(X_df, y_ser, lookback):\n",
    "    Xv = X_df.values.astype(np.float32)\n",
    "    yv = y_ser.values.astype(np.int32)\n",
    "    xs, ys = [], []\n",
    "    for i in range(lookback-1, len(X_df)):\n",
    "        xs.append(Xv[i - lookback + 1 : i + 1])\n",
    "        ys.append(yv[i])\n",
    "    return np.stack(xs, axis=0), np.array(ys)\n",
    "\n",
    "def build_model(n_features, width1=64, width2=32, dropout=0.10, lr=5e-4, use_gru=True):\n",
    "    rnn = layers.GRU if use_gru else layers.LSTM\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=(None, n_features)),\n",
    "        rnn(width1, return_sequences=True, recurrent_dropout=dropout),\n",
    "        layers.LayerNormalization(),\n",
    "        rnn(width2, recurrent_dropout=dropout),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.Dense(16, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-5)),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ])\n",
    "    m.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.AUC(name=\"auc\"),\n",
    "                 tf.keras.metrics.AUC(name=\"auprc\", curve=\"PR\")]\n",
    "    )\n",
    "    return m\n",
    "\n",
    "# CHANGE: MCC @ best threshold (auf Validation) – nicht fix 0.5\n",
    "def mcc_at_best_thr(y_true, y_prob):\n",
    "    ts = np.r_[0.0, np.unique(y_prob), 1.0]\n",
    "    best = (-1.0, 0.5)\n",
    "    for t in ts:\n",
    "        m = matthews_corrcoef(y_true, (y_prob >= t).astype(int))\n",
    "        if m > best[0]:\n",
    "            best = (float(m), float(t))\n",
    "    return best  # (mcc, thr)\n",
    "\n",
    "def fit_eval_fold(X_tr, y_tr, X_va, y_va, lookback, hp, seed=SEED, batch=BATCH, epochs=EPOCHS_GRID):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Scaler nur auf Train\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_s = pd.DataFrame(scaler.fit_transform(X_tr), index=X_tr.index, columns=X_tr.columns)\n",
    "    X_va_s = pd.DataFrame(scaler.transform(X_va),     index=X_va.index, columns=X_va.columns)\n",
    "\n",
    "    Xtr_win, ytr = make_windows(X_tr_s, y_tr, lookback)\n",
    "    Xva_win, yva = make_windows(X_va_s, y_va, lookback)\n",
    "\n",
    "    ds_tr = tf.data.Dataset.from_tensor_slices((Xtr_win, ytr)) \\\n",
    "            .shuffle(len(Xtr_win), seed=seed) \\\n",
    "            .batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    ds_va = tf.data.Dataset.from_tensor_slices((Xva_win, yva)) \\\n",
    "            .batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    model = build_model(\n",
    "        n_features=Xtr_win.shape[-1],\n",
    "        width1=hp[\"width1\"], width2=hp[\"width2\"],\n",
    "        dropout=hp[\"dropout\"], lr=hp[\"lr\"], use_gru=(hp[\"cell\"]==\"GRU\")\n",
    "    )\n",
    "\n",
    "    cbs = [\n",
    "        callbacks.EarlyStopping(monitor=\"val_auprc\", mode=\"max\", patience=6, restore_best_weights=True),\n",
    "        callbacks.ReduceLROnPlateau(monitor=\"val_auprc\", mode=\"max\", factor=0.5, patience=3, min_lr=1e-5),\n",
    "        callbacks.TerminateOnNaN(),\n",
    "    ]\n",
    "\n",
    "    print(f\"  -> fit (epochs={epochs}, batch={batch}) ...\", flush=True)\n",
    "    hist = model.fit(ds_tr, validation_data=ds_va, epochs=epochs, verbose=1 if FAST else 0, callbacks=cbs)\n",
    "\n",
    "    yva_proba = model.predict(ds_va, verbose=0).ravel()\n",
    "\n",
    "    # ... nach dem Val-Predict:\n",
    "    mcc_val, thr_val = mcc_at_best_thr(yva, yva_proba)   # wie in deiner Suche\n",
    "    yva_pred_best = (yva_proba >= thr_val).astype(int)\n",
    "\n",
    "    metrics = dict(\n",
    "        mcc=float(mcc_val),\n",
    "        thr_val=float(thr_val),\n",
    "        bal_acc=float(balanced_accuracy_score(yva, yva_pred_best)),  # <— zurück\n",
    "        auprc=float(average_precision_score(yva, yva_proba)),\n",
    "        auroc=float(roc_auc_score(yva, yva_proba)),\n",
    "        epochs_trained=int(len(hist.history[\"loss\"]))\n",
    "    )\n",
    "    tf.keras.backend.clear_session()\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "337cbd2c-95af-41f5-9be0-dd323baf6fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check vor der Suche:\n",
      "  n Zeilen df: 3402\n",
      "  FEATURESET: v2\n",
      "  FEATURES_ALL: 11\n",
      "  FEATURE_SUBSETS: ['all', 'mom_only', 'mom+vol']\n",
      "  LOOKBACK_GRID: [40, 60, 120]\n",
      "  HP_GRID: 24\n",
      "  #splits: 5\n",
      "  Fold0 sizes (train/val): 1531 680\n"
     ]
    }
   ],
   "source": [
    "# --- Grids --------------------------------------------------------------\n",
    "# CHANGE: aussagekräftiges, aber kompaktes Grid (per Spezifikation)\n",
    "LOOKBACK_GRID = [40, 60, 120] if not FAST else [LOOKBACK_DEFAULT]\n",
    "HP_GRID = [\n",
    "    dict(width1=w1, width2=w2, dropout=dp, lr=lr, cell=cell)\n",
    "    for (w1, w2) in [(32,16), (64,32)]\n",
    "    for lr in [5e-4, 3e-4]\n",
    "    for dp in [0.0, 0.1, 0.2]\n",
    "    for cell in [\"GRU\", \"LSTM\"]\n",
    "] if not FAST else [dict(width1=32, width2=16, dropout=0.10, lr=5e-4, cell=\"GRU\")]\n",
    "\n",
    "# CHANGE: neues Feature-Subset „mom+vol“\n",
    "FEATURE_SUBSETS = {\n",
    "    \"all\": FEATURES_ALL,\n",
    "    \"mom_only\": [c for c in FEATURES_ALL\n",
    "                 if (\"logret\" in c) or (\"macd\" in c) or (c in {\"sma_diff\",\"rsi_14\",\"bb_pos\"})],\n",
    "    \"mom+vol\": [c for c in FEATURES_ALL\n",
    "                if ((\"logret\" in c) or (\"macd\" in c) or (c in {\"sma_diff\",\"rsi_14\",\"bb_pos\"}))\n",
    "                   or (c in {\"realized_vol_10\",\"vol_z_20\"})]   # <- Ergänzung\n",
    "}\n",
    "\n",
    "print(\"Sanity check vor der Suche:\")\n",
    "print(\"  n Zeilen df:\", len(df))\n",
    "print(\"  FEATURESET:\", FEATURESET)\n",
    "print(\"  FEATURES_ALL:\", len(FEATURES_ALL))\n",
    "print(\"  FEATURE_SUBSETS:\", list(FEATURE_SUBSETS.keys()))\n",
    "print(\"  LOOKBACK_GRID:\", LOOKBACK_GRID)\n",
    "print(\"  HP_GRID:\", (len(HP_GRID) if not FAST else \"FAST=1\"))\n",
    "print(\"  #splits:\", len(splits))\n",
    "if len(splits) > 0:\n",
    "    tr_s, va_s = splits[0]\n",
    "    print(\"  Fold0 sizes (train/val):\", tr_s.stop, va_s.stop - va_s.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42408c93-f83a-4abc-ab98-1d0279037343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte FAST-Suche ...\n",
      "[FAST] Grid: subsets=['mom+vol'], LB=[60], HPs=1, folds=3\n",
      "[FAST] Budget: 60 min, EPOCHS=25, BATCH=128\n",
      "\n",
      "  -> fit (epochs=25, batch=128) ...\n",
      "[mom+vol | LB=60 | GRU 32/16 dp=0.1 lr=0.0005] Fold1: MCC@best=0.106 thr=0.635 AUPRC=0.562\n",
      "  -> fit (epochs=25, batch=128) ...\n",
      "[mom+vol | LB=60 | GRU 32/16 dp=0.1 lr=0.0005] Fold2: MCC@best=0.051 thr=0.628 AUPRC=0.513\n",
      "  -> fit (epochs=25, batch=128) ...\n",
      "[mom+vol | LB=60 | GRU 32/16 dp=0.1 lr=0.0005] Fold3: MCC@best=0.041 thr=0.418 AUPRC=0.501\n",
      "--> Summary [mom+vol | LB=60 | GRU 32/16] ran_folds=3/3\n",
      "\n",
      "Suche fertig: combos=1, records=3, Dauer=15.2s\n",
      "Geschrieben: ..\\results\\2025-10-22_12-13-21_wfcv\\wfcv_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ---- Suche (FAST PATCH) ------------------------------------------------------\n",
    "from time import perf_counter\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Starte FAST-Suche ...\", flush=True)\n",
    "\n",
    "# ---------- Fast-Overrides nur für diese Zelle ----------\n",
    "FAST_PATCH     = True\n",
    "MAX_SECONDS    = 60 * 60        # 60 min Zeitbudget\n",
    "EPOCHS_FAST    = 25             # statt 60\n",
    "NFOLDS_FAST    = 3              # statt 5\n",
    "BATCH_FAST     = 128            # größerer Batch -> weniger Steps/Epoche\n",
    "\n",
    "# reduziere Subsets/Lookbacks/HPs radikal für <~1h auf CPU\n",
    "FEATURE_SUBSETS_FAST = {}\n",
    "# nimm bevorzugt \"mom+vol\", sonst \"mom_only\", sonst 1. Eintrag:\n",
    "if \"mom+vol\" in FEATURE_SUBSETS:\n",
    "    FEATURE_SUBSETS_FAST[\"mom+vol\"] = FEATURE_SUBSETS[\"mom+vol\"]\n",
    "elif \"mom_only\" in FEATURE_SUBSETS:\n",
    "    FEATURE_SUBSETS_FAST[\"mom_only\"] = FEATURE_SUBSETS[\"mom_only\"]\n",
    "else:\n",
    "    k0 = next(iter(FEATURE_SUBSETS.keys()))\n",
    "    FEATURE_SUBSETS_FAST[k0] = FEATURE_SUBSETS[k0]\n",
    "\n",
    "LOOKBACK_GRID_FAST = [60]       # ein Lookback\n",
    "HP_GRID_FAST = [                # eine solide, schnelle Kombi\n",
    "    dict(width1=32, width2=16, dropout=0.10, lr=5e-4, cell=\"GRU\"),\n",
    "]\n",
    "\n",
    "# Wende Overrides an (nur in dieser Zelle)\n",
    "if FAST_PATCH:\n",
    "    EPOCHS_GRID = EPOCHS_FAST\n",
    "    # begrenze auf die ersten N_FOLDS\n",
    "    splits = splits[:min(NFOLDS_FAST, len(splits))]\n",
    "    FEATURE_SUBSETS = FEATURE_SUBSETS_FAST\n",
    "    LOOKBACK_GRID = LOOKBACK_GRID_FAST\n",
    "    HP_GRID = HP_GRID_FAST\n",
    "\n",
    "# ---------- Resume-Unterstützung & Sofort-Checkpoint ----------\n",
    "csv_path = RUN_DIR / \"wfcv_results.csv\"\n",
    "records = []\n",
    "total_combos = 0\n",
    "t0 = perf_counter()\n",
    "\n",
    "# bereits existierende Ergebnisse zum Skipping laden\n",
    "if csv_path.exists():\n",
    "    done_df = pd.read_csv(csv_path)                    # <-- IMMER definieren\n",
    "    done_keys = {\n",
    "        (r[\"features_used\"], int(r[\"lookback\"]),\n",
    "         r[\"cell\"], int(r[\"width1\"]), int(r[\"width2\"]),\n",
    "         float(r[\"dropout\"]), float(r[\"lr\"]), int(r[\"fold\"]))\n",
    "        for _, r in done_df.iterrows()\n",
    "    }\n",
    "else:\n",
    "    done_df = pd.DataFrame()                           # <-- leerer Frame\n",
    "    done_keys = set()\n",
    "\n",
    "\n",
    "def _size(slc):  # helper\n",
    "    return slc.stop - (slc.start or 0)\n",
    "\n",
    "stop_time = t0 + MAX_SECONDS\n",
    "print(f\"[FAST] Grid: subsets={list(FEATURE_SUBSETS.keys())}, LB={LOOKBACK_GRID}, HPs={len(HP_GRID)}, folds={len(splits)}\")\n",
    "print(f\"[FAST] Budget: {MAX_SECONDS//60:.0f} min, EPOCHS={EPOCHS_GRID}, BATCH={BATCH_FAST}\\n\")\n",
    "\n",
    "for feat_name, FEATS in FEATURE_SUBSETS.items():\n",
    "    if len(FEATS) == 0:\n",
    "        print(f\"[{feat_name}] übersprungen: 0 Features.\")\n",
    "        continue\n",
    "\n",
    "    for lookback in LOOKBACK_GRID:\n",
    "        for hp in HP_GRID:\n",
    "            total_combos += 1\n",
    "            ran_folds = 0\n",
    "\n",
    "            for fold_id, (tr_s, va_s) in enumerate(splits, start=1):\n",
    "                key = (feat_name, int(lookback),\n",
    "                       hp[\"cell\"], int(hp[\"width1\"]), int(hp[\"width2\"]),\n",
    "                       float(hp[\"dropout\"]), float(hp[\"lr\"]), int(fold_id))\n",
    "                if key in done_keys:\n",
    "                    print(f\"[{feat_name} | LB={lookback} | {hp['cell']} {hp['width1']}/{hp['width2']} \"\n",
    "                          f\"dp={hp['dropout']} lr={hp['lr']}] Fold{fold_id}: SKIP (bereits vorhanden)\")\n",
    "                    ran_folds += 1\n",
    "                    continue\n",
    "\n",
    "                    # Zeitbudget prüfen\n",
    "                if perf_counter() > stop_time:\n",
    "                    print(\"[INFO] Zeitbudget erreicht — speichere & beende.\")\n",
    "                    # vorhandene + neue records sicher schreiben\n",
    "                    out_df = pd.concat([done_df, pd.DataFrame.from_records(records)], ignore_index=True) if csv_path.exists() else pd.DataFrame.from_records(records)\n",
    "                    out_df.to_csv(csv_path, index=False)\n",
    "                    raise SystemExit(0)\n",
    "\n",
    "                X_tr, y_tr = X_full.iloc[tr_s][FEATS], y_full.iloc[tr_s]\n",
    "                X_va, y_va = X_full.iloc[va_s][FEATS], y_full.iloc[va_s]\n",
    "\n",
    "                # Minimalcheck für Windowing\n",
    "                if len(X_tr) < (lookback + 50) or len(X_va) < (lookback + 10):\n",
    "                    print(f\"[{feat_name} | LB={lookback} | {hp['cell']} {hp['width1']}/{hp['width2']} \"\n",
    "                          f\"dp={hp['dropout']} lr={hp['lr']}] Skip-Fold (zu kurz): \"\n",
    "                          f\"train={len(X_tr)}, val={len(X_va)}, needed>={(lookback+50)}/{(lookback+10)}\")\n",
    "                    continue\n",
    "\n",
    "                # NOTE: wir erzwingen hier batch=BATCH_FAST und epochs=EPOCHS_GRID\n",
    "                mets = fit_eval_fold(X_tr, y_tr, X_va, y_va,\n",
    "                                     lookback=lookback, hp=hp,\n",
    "                                     epochs=EPOCHS_GRID, batch=BATCH_FAST)\n",
    "\n",
    "                ran_folds += 1\n",
    "                rec = {\n",
    "                    \"feature_set\": FEATURESET,\n",
    "                    \"features_used\": feat_name,\n",
    "                    \"n_features\": len(FEATS),\n",
    "                    \"lookback\": lookback,\n",
    "                    **hp,\n",
    "                    \"fold\": fold_id,\n",
    "                    **mets\n",
    "                }\n",
    "                records.append(rec)\n",
    "\n",
    "                # Sofort persistieren (robust bei Abbruch)\n",
    "                cur_df = pd.DataFrame.from_records(records)\n",
    "                out_df = pd.concat([done_df, cur_df], ignore_index=True)\n",
    "                out_df.to_csv(csv_path, index=False)\n",
    "                done_df = out_df                                      # <-- in-memory aktualisieren\n",
    "\n",
    "                # Log – expects mets['mcc'] und mets['thr_val'] (aus mcc_at_best_thr)\n",
    "                thr_str = f\"{mets.get('thr_val', 0.5):.3f}\" if 'thr_val' in mets else \"n/a\"\n",
    "                print(f\"[{feat_name} | LB={lookback} | {hp['cell']} {hp['width1']}/{hp['width2']} \"\n",
    "                      f\"dp={hp['dropout']} lr={hp['lr']}] Fold{fold_id}: \"\n",
    "                      f\"MCC@best={mets['mcc']:.3f} thr={thr_str} AUPRC={mets['auprc']:.3f}\")\n",
    "\n",
    "            print(f\"--> Summary [{feat_name} | LB={lookback} | {hp['cell']} {hp['width1']}/{hp['width2']}] \"\n",
    "                  f\"ran_folds={ran_folds}/{len(splits)}\")\n",
    "\n",
    "t1 = perf_counter()\n",
    "print(f\"\\nSuche fertig: combos={total_combos}, records={len(records)}, Dauer={t1-t0:.1f}s\")\n",
    "\n",
    "# finaler Merge (falls während der Suche schon geschrieben wurde)\n",
    "final_df = pd.read_csv(csv_path) if csv_path.exists() else pd.DataFrame.from_records(records)\n",
    "final_df.to_csv(csv_path, index=False)\n",
    "print(\"Geschrieben:\", csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2afca6e0-5c56-45c9-93a0-066fc19b613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config: {'feature_set': 'v2', 'features_used': 'mom+vol', 'n_features': 11, 'lookback': 60, 'width1': 32, 'width2': 16, 'dropout': 0.1, 'lr': 0.0005, 'cell': 'GRU', 'mcc_mean': 0.065751403238187, 'mcc_std': 0.03483550388528836, 'auprc_mean': 0.52533897424432, 'auprc_std': 0.032371146777913404, 'auroc_mean': 0.51403827184167, 'balacc_mean': 0.527664905383352, 'n_folds': 3}\n",
      "→ geschrieben: ..\\results\\2025-10-22_12-13-21_wfcv\\wfcv_results_agg.csv und ..\\results\\2025-10-22_12-13-21_wfcv\\best_config.json\n"
     ]
    }
   ],
   "source": [
    "# ---- Aggregation & Best-Config (robust aus CSV) ------------------------------\n",
    "import pandas as pd, json, numpy as np\n",
    "\n",
    "csv_path = RUN_DIR / \"wfcv_results.csv\"\n",
    "assert csv_path.exists(), f\"wfcv_results.csv fehlt unter {csv_path} – zuerst die Suche laufen lassen.\"\n",
    "\n",
    "results = pd.read_csv(csv_path)\n",
    "\n",
    "# Dedupe (Resume-Fälle)\n",
    "key_cols = [\"features_used\",\"lookback\",\"cell\",\"width1\",\"width2\",\"dropout\",\"lr\",\"fold\"]\n",
    "present_keys = [c for c in key_cols if c in results.columns]\n",
    "results = results.drop_duplicates(subset=present_keys, keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "# Typen glätten (falls Strings reingerutscht sind)\n",
    "for col in [\"lookback\",\"width1\",\"width2\",\"fold\"]:\n",
    "    if col in results.columns:\n",
    "        results[col] = pd.to_numeric(results[col], errors=\"coerce\").astype(\"Int64\")\n",
    "for col in [\"dropout\",\"lr\"]:\n",
    "    if col in results.columns:\n",
    "        results[col] = pd.to_numeric(results[col], errors=\"coerce\")\n",
    "\n",
    "# Pflichtmetriken\n",
    "need_cols = {\"mcc\",\"auprc\",\"auroc\"}\n",
    "missing = need_cols - set(results.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Fehlende Spalten in wfcv_results.csv: {sorted(missing)}\")\n",
    "\n",
    "# Gruppierschlüssel (nur vorhandene)\n",
    "agg_cols = [c for c in [\"feature_set\",\"features_used\",\"n_features\",\"lookback\",\n",
    "                        \"width1\",\"width2\",\"dropout\",\"lr\",\"cell\"] if c in results.columns]\n",
    "\n",
    "# Aggregation; bal_acc optional\n",
    "agg_dict = {\"mcc\": [\"mean\",\"std\"], \"auprc\": [\"mean\",\"std\"], \"auroc\": [\"mean\"]}\n",
    "if \"bal_acc\" in results.columns:\n",
    "    agg_dict[\"bal_acc\"] = [\"mean\"]\n",
    "\n",
    "g = results.groupby(agg_cols).agg(agg_dict)\n",
    "\n",
    "# MultiIndex-Spalten flatten (robust)\n",
    "g.columns = [\n",
    "    \"_\".join([str(x) for x in col if str(x) != \"\"]).strip(\"_\")\n",
    "    for col in g.columns.to_flat_index()\n",
    "]\n",
    "g = g.reset_index()\n",
    "\n",
    "# n_folds sauber mergen (statt .values)\n",
    "nf = results.groupby(agg_cols).size().reset_index(name=\"n_folds\")\n",
    "g = g.merge(nf, on=agg_cols, how=\"left\")\n",
    "\n",
    "# Benennungen hübsch (nur falls nötig)\n",
    "rename_map = {\n",
    "    \"mcc_mean\":\"mcc_mean\", \"mcc_std\":\"mcc_std\",\n",
    "    \"auprc_mean\":\"auprc_mean\", \"auprc_std\":\"auprc_std\",\n",
    "    \"auroc_mean\":\"auroc_mean\", \"bal_acc_mean\":\"balacc_mean\"\n",
    "}\n",
    "g = g.rename(columns=rename_map)\n",
    "\n",
    "# Ranking\n",
    "g = g.sort_values([\"mcc_mean\",\"auprc_mean\",\"mcc_std\"], ascending=[False, False, True])\n",
    "\n",
    "# Artefakte\n",
    "(g).to_csv(RUN_DIR / \"wfcv_results_agg.csv\", index=False)\n",
    "top5 = g.head(5).copy()\n",
    "top5.to_csv(RUN_DIR / \"wfcv_results_top5.csv\", index=False)\n",
    "\n",
    "best = top5.iloc[0].to_dict() if len(top5) else {}\n",
    "with open(RUN_DIR / \"best_config.json\", \"w\") as f:\n",
    "    json.dump(best, f, indent=2)\n",
    "\n",
    "print(\"Best config:\", best)\n",
    "print(\"→ geschrieben:\", RUN_DIR / \"wfcv_results_agg.csv\", \"und\", RUN_DIR / \"best_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a79f67d2-a756-442f-8aaa-047113714208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Plots geschrieben: ..\\results\\2025-10-22_12-13-21_wfcv\\plots\\score_grid_mcc.png  &  ..\\results\\2025-10-22_12-13-21_wfcv\\plots\\score_grid_auprc.png\n"
     ]
    }
   ],
   "source": [
    "# ---- Score-Grids als Plot (robust) ------------------------------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "agg_path = RUN_DIR / \"wfcv_results_agg.csv\"\n",
    "assert agg_path.exists(), f\"wfcv_results_agg.csv fehlt: {agg_path}\"\n",
    "agg = pd.read_csv(agg_path)   # <-- explizit laden (nicht auf 'g' verlassen)\n",
    "\n",
    "# wähle sinnvolle Spalten-Kombinationen, die es wirklich gibt\n",
    "pivot_index = \"lookback\" if \"lookback\" in agg.columns else None\n",
    "col_candidates = [\"features_used\", \"cell\", \"width1\"]\n",
    "pivot_columns = [c for c in col_candidates if c in agg.columns]\n",
    "if not pivot_columns:  # Fallback, falls fast-Grid sehr klein ist\n",
    "    pivot_columns = [agg.columns[0]]\n",
    "\n",
    "(RUN_DIR / \"plots\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _plot_grid(df: pd.DataFrame, value_col: str, fname: str):\n",
    "    if value_col not in df.columns:\n",
    "        print(f\"[WARN] Spalte {value_col} fehlt – skip Plot {fname}\")\n",
    "        return\n",
    "    idx = pivot_index or pivot_columns[0]\n",
    "    pvt = df.pivot_table(index=idx, columns=pivot_columns, values=value_col, aggfunc=\"mean\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    im = plt.imshow(pvt.values, aspect=\"auto\")\n",
    "    plt.colorbar(im)\n",
    "\n",
    "    # Achsen-Beschriftungen\n",
    "    plt.yticks(range(len(pvt.index)), [str(x) for x in pvt.index])\n",
    "    if isinstance(pvt.columns, pd.MultiIndex):\n",
    "        xt = [\" | \".join(str(x) for x in tup) for tup in pvt.columns.to_list()]\n",
    "    else:\n",
    "        xt = [str(x) for x in pvt.columns.to_list()]\n",
    "    plt.xticks(range(pvt.shape[1]), xt, rotation=45, ha=\"right\")\n",
    "\n",
    "    plt.title(fname.replace(\"_\", \" \").replace(\".png\", \"\"))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RUN_DIR / \"plots\" / fname, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "# Heatmaps zeichnen (nur, wenn die Zielspalten vorhanden sind)\n",
    "_plot_grid(agg, \"mcc_mean\",   \"score_grid_mcc.png\")\n",
    "_plot_grid(agg, \"auprc_mean\", \"score_grid_auprc.png\")\n",
    "print(\"✓ Plots geschrieben:\", RUN_DIR / \"plots\" / \"score_grid_mcc.png\",\n",
    "      \" & \", RUN_DIR / \"plots\" / \"score_grid_auprc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "103d010d-c556-4942-a47e-dc5f79ae5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Boxplots über Folds (MCC/AUPRC) -----------------------------------\n",
    "# CHANGE: pro Konfiguration (kompakter Label) Boxplots der Fold-Verteilung speichern\n",
    "def _short_label(r):\n",
    "    return f\"{r['features_used']}-{r['cell']}-{int(r['width1'])}/{int(r['width2'])}-lb{int(r['lookback'])}-dp{r['dropout']}-lr{r['lr']}\"\n",
    "\n",
    "if not results.empty:\n",
    "    results[\"config_label\"] = results.apply(_short_label, axis=1)\n",
    "\n",
    "    # MCC\n",
    "    plt.figure(figsize=(max(8, 0.35*len(results[\"config_label\"].unique())), 5))\n",
    "    data = [grp[\"mcc\"].values for _, grp in results.groupby(\"config_label\")]\n",
    "    labels = list(results.groupby(\"config_label\").groups.keys())\n",
    "    plt.boxplot(data, showmeans=True, meanline=True)\n",
    "    plt.xticks(range(1, len(labels)+1), labels, rotation=90)\n",
    "    plt.title(\"MCC über Folds (pro Konfiguration)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RUN_DIR / \"plots\" / \"boxplots_mcc.png\", dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "    # AUPRC\n",
    "    plt.figure(figsize=(max(8, 0.35*len(results[\"config_label\"].unique())), 5))\n",
    "    data = [grp[\"auprc\"].values for _, grp in results.groupby(\"config_label\")]\n",
    "    labels = list(results.groupby(\"config_label\").groups.keys())\n",
    "    plt.boxplot(data, showmeans=True, meanline=True)\n",
    "    plt.xticks(range(1, len(labels)+1), labels, rotation=90)\n",
    "    plt.title(\"AUPRC über Folds (pro Konfiguration)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RUN_DIR / \"plots\" / \"boxplots_auprc.png\", dpi=160)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5581e69e-5b28-4224-9026-2872d2313683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block 5 abgeschlossen. Artefakte:\n",
      " - ..\\results\\2025-10-22_12-13-21_wfcv\\wfcv_results.csv\n",
      " - ..\\results\\2025-10-22_12-13-21_wfcv\\wfcv_results_agg.csv\n",
      " - ..\\results\\2025-10-22_12-13-21_wfcv\\best_config.json\n",
      " - ..\\results\\2025-10-22_12-13-21_wfcv\\wfcv_results_top5.csv\n",
      " - ..\\results\\2025-10-22_12-13-21_wfcv\\plots\n"
     ]
    }
   ],
   "source": [
    "# ---- Run-Info dump ------------------------------------------------------\n",
    "run_info = {\n",
    "    \"seed\": SEED,\n",
    "    \"epochs_grid\": EPOCHS_GRID,\n",
    "    \"n_folds\": N_FOLDS,\n",
    "    \"val_frac\": 0.20,\n",
    "    \"min_train_frac\": 0.45,\n",
    "    \"lookback_grid\": LOOKBACK_GRID,\n",
    "    \"hp_grid_size\": (len(HP_GRID) if not FAST else 1),\n",
    "    \"feature_subsets\": list(FEATURE_SUBSETS.keys()),\n",
    "    \"train_csv\": TRAIN_CSV,\n",
    "    \"label_resolution\": {\n",
    "        \"source\": \"yaml\" if os.path.exists(yaml_path) and (label_h is not None) else \"inferred_from_csv\",\n",
    "        \"yaml_path\": yaml_path\n",
    "    },\n",
    "    \"labels\": {\"horizon\": H_FOR_FILE, \"mode\": MODE_FOR_FILE, \"epsilon\": EPS_FOR_FILE},\n",
    "    \"notes\": {\n",
    "        \"metric_mcc\": \"Validation MCC at best threshold per fold (not fixed 0.5)\",  # CHANGE\n",
    "        \"boxplots\": [\"plots/boxplots_mcc.png\", \"plots/boxplots_auprc.png\"]          # CHANGE\n",
    "    }\n",
    "}\n",
    "with open(RUN_DIR / \"wfcv_run_info.json\", \"w\") as f:\n",
    "    json.dump(run_info, f, indent=2)\n",
    "\n",
    "print(\"\\nBlock 5 abgeschlossen. Artefakte:\")\n",
    "print(\" -\", RUN_DIR / \"wfcv_results.csv\")\n",
    "print(\" -\", RUN_DIR / \"wfcv_results_agg.csv\")\n",
    "print(\" -\", RUN_DIR / \"best_config.json\")\n",
    "print(\" -\", RUN_DIR / \"wfcv_results_top5.csv\")\n",
    "print(\" -\", RUN_DIR / \"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6703011-38bf-428d-b1b5-a9537e1903d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (finance-lstm)",
   "language": "python",
   "name": "finance-lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
